%%% Title:    DSS Stats & Methods: Lecture 10
%%% Author:   Kyle M. Lang
%%% Created:  2017-SEP-12
%%% Modified: 2018-AUG-24

\documentclass{beamer}
\usetheme[%
  pageofpages          = of,
  bullet               = circle,
  titleline            = true,
  alternativetitlepage = true,
  titlepagelogo        = Logo3,
  watermark            = watermarkTiU,
  watermarkheight      = 100px,
  watermarkheightmult  = 4%
]{UVT}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{fancybox}
\usepackage[mathcal]{eucal}

\newcommand{\kfold}[0]{\emph{K}-fold cross-validation}

%% Ensure styles of `blocks' (used in Definitions, Theorems etc.) follows the
%% UVT-style theme:
\setbeamercolor{block title}{fg = darkblue, bg = white}
\setbeamercolor{block body}{use = block title, bg = block title.bg}

%% Ensure TableOfContents is in UVT-style theme:
\setbeamercolor{section in toc}{fg = darkblue}

\title{Variable Selection \& Model Regularization}
\subtitle{Statistics \& Methodology Lecture 10}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Tilburg University}
\date{XX October 2018}

\begin{document}

%------------------------------------------------------------------------------%

<<setup, include=FALSE>>=
set.seed(235711)

library(knitr)
library(ggplot2)
library(MASS)
library(mvtnorm)
library(glmnet)
library(xtable)

source("../../../code/supportFunctions.R")
dataDir <- "../data/"

opts_chunk$set(size = 'footnotesize', fig.align = 'center')
knit_theme$set('edit-kwrite')

lightBlue <- rgb(0, 137, 191, max = 255)
midBlue   <- rgb(0, 131, 183, max = 255)
darkBlue  <- rgb(0, 128, 179, max = 255)
deepGold  <- rgb(184, 138, 45, max = 255)
lightGold <- rgb(195, 146, 48, max = 255)
@

%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
\titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  
  \begin{enumerate}
  \item Variable selection
  \item Model regularization
  \item Collinearity
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Challenges in Prediction}
    
  In real-world prediction problems, we're faced with two principal
  difficulties:
    \vb
    \begin{enumerate}
    \item Specifying the predictor set of your regression model to optimize 
      prediction performance.
      \vb
    \item Controlling the variance in the fitted solution so that you can be
      confident in the stability of your out-of-sample predictions.
    \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Challenges in Prediction}
  
  For best prediction performance, you need to balance two, somewhat
  contradictory, goals:
  \vb
  \begin{itemize}
  \item Include as many predictors (and interaction terms, polynomial terms,
    etc.) as possible.
    \vb
    \begin{itemize}
    \item Trying to avoid \emph{omitted variable bias}.
    \end{itemize}
    \vb
  \item Only include predictors that are strongly related to the outcome.
    \vb
    \begin{itemize}
    \item Adding poor predictors only increases the noise in the predictions.
    \end{itemize}
  \end{itemize}
  \vb 
  \emph{Regularized regression} methods can be greatly helpful in balancing
  these conflicting goals.
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{}
  
  \begin{center}
    \Huge{\textsc{Variable Selection}}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Top-Down, Theory-Driven}
  
  In some situations, we may have theoretically defined sets of predictors that 
  we can compare.
  \vb
  \begin{itemize}
  \item We simply compare the fit of the candidate models.
    \begin{itemize}
    \item If the models are nested, we can conduct a $\Delta R^2$ test.
    \item We can always compare MSE values.
    \item We can also compare information criteria.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Information Criteria}
  
  Information criteria (ICs) are measures of model fit based on the maximized 
  \emph{log-likelihood} function.
  \vc
  \begin{itemize}
  \item ICs attempt to penalize the fit for model complexity using only training data
    \vc
  \item ICs can only be used for model comparison; they are not independently interpretable
  \end{itemize}
  
  \va
  \pause
  
  There are many ICs, but the two most popular are:
  \vc
  \begin{itemize}
  \item Akaike Information Criterion (AIC):
    \begin{align*}
      AIC = 2K - 2\hat{\mathcal{L}}
    \end{align*}
  \item Bayesian Information Criterion (BIC):
    \begin{align*}
      BIC = K\ln N - 2\hat{\mathcal{L}}
    \end{align*}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Aside: Likelihoods}
  
  Suppose we have the following model:
  \begin{align*}
    Y = \beta_0 + \beta_1 X + \varepsilon,~~~~
    \varepsilon \sim \text{N}\left( 0, \sigma^2 \right).
  \end{align*}
  This model can be equivelently written as:
  \begin{align*}
    Y \sim \text{N} \left( \beta_0 + \beta_1 X, \sigma^2 \right)
  \end{align*}
  or, for a given $\{Y_n, X_n\}$, as:
  \begin{align}
    P \left( Y_n|X_n, \beta_0, \beta_1, \sigma^2 \right) = 
    \frac{1}{\sqrt{2 \pi \sigma^2}} 
    e^{-\frac{\left( Y_n - \beta_0 - \beta_1 X_n \right)^2}{2\sigma^2}}. \label{olsPdf}
  \end{align}
  
  If we plug our estimated parameters into Equation \ref{olsPdf}, we get the 
  probability of observing $Y_n$ given $\hat{Y}_n$ and $\hat{\sigma}^2$.
  \begin{align}
    P \left( Y_n|X_n, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}^2 \right) = 
    \frac{1}{\sqrt{2 \pi \hat{\sigma}^2}} 
    e^{-\frac{\left( Y_n - \hat{\beta}_0 - \hat{\beta}_1 X_n \right)^2}{2\hat{\sigma}^2}} \label{estOlsPdf}
  \end{align}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Aside: Likelihoods}

  Applying Equation \ref{estOlsPdf} to all $N$ observations and multiplying the 
  results produces a \emph{likelihood}:
  \begin{align*}
    \hat{L} \left( \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}^2 \right) = 
    \prod_{n = 1}^N P \left( Y_n|X_n, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}^2 \right)
  \end{align*}
  
  We generally want to work with the natural logarithm of Equation 
  \ref{estOlsPdf}. Doing so gives the \emph{loglikelihood}:
  \begin{align*}
  \hat{\mathcal{L}} \left( \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}^2 \right) &= 
    \ln \prod_{n = 1}^N P \left( Y_n|X_n, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}^2 \right)\\ 
    &= -\frac{N}{2} \ln 2\pi - N \ln \sigma - \frac{1}{2\sigma^2} 
    \sum_{n = 1}^N \left( Y_n - \hat{\beta}_0 - \hat{\beta}_1 X_n \right)^2
  \end{align*}
  
\end{frame}
  
\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}
    
<<echo = FALSE>>=
diabetes <- readRDS("../data/diabetes.rds")
@ 

<<>>=
## Fit a model:
out1 <- lm(ldl ~ bp + glu + bmi, data = diabetes)

## Extract the predicted values and estimated residual 
## standard error:
yHat <- predict(out1)
s    <- summary(out1)$sigma

## Compute the row-wise probabilities:
pY <- dnorm(diabetes$ldl, mean = yHat, sd = s)

## Compute the loglikelihood:
ll1 <- sum(log(pY))
@

\pagebreak

<<>>=
## Define constants:
K <- length(coef(out1)) + 1
N <- nrow(diabetes)

## Compute AIC:
aic1 <- 2 * K - 2 * ll1

## Compute BIC:
bic1 <- K * log(N) - 2 * ll1

## Get R's built-in versions
ll0  <- logLik(out1)
aic0 <- AIC(out1)
bic0 <- BIC(out1)
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Example}
  
  Finally, let's compare the manual and built-in approaches:
  
<<echo = FALSE, results = "asis">>=
tab <- matrix(c(ll0, ll1, aic0, aic1, bic0, bic1),
              nrow = 2,
              dimnames = list(c("Built-In", "Manual"), c("LL", "AIC", "BIC"))
              )
print(xtable(tab), digits = 2, booktabs = TRUE)
@

\end{frame}


\begin{frame}[fragile]{Example}
  
<<echo = FALSE>>=
dDat <- readRDS("../data/diabetes.rds") 
@ 

<<>>=
## Fit the models:
outF <- lm(bp ~ age + bmi + ldl + hdl, data = dDat)
outR <- lm(bp ~ age + bmi, data = dDat)

## Compute ICs:
aicF <- AIC(outF); aicR <- AIC(outR)

bicF <- BIC(outF); bicR <- BIC(outR)
@ 

<<echo = FALSE, results = "asis">>=
tab1 <- xtable(
    matrix(c(aicF, bicF, aicR, bicR), 
           ncol = 2, 
           dimnames = list(c("AIC", "BIC"), c("Full", "Restricted"))
           ),
    caption = "Information Criteria")

print(tab1, digits = 2, booktabs = TRUE)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Best Subset Selection}
  
  The idea of best subset selection (BSS) is to build a good model by trying all 
  possible subsets of the candidate predictors.
  \vb
  \begin{itemize}
  \item Given a set of $K$ candidate predictors:
    \vb
    \begin{enumerate}
    \item For each value of $k = 0, 1, 2, \ldots, K$, regress $Y$ onto all 
      $K \choose k$ possible subsets of $k$  
      predictors \label{bssStep1}
      \vb
    \item Find the best subset of size $k$ by selecting the model from Step 
      \ref{bssStep1} with the largest $R^2$ (or smallest MSE) \label{bssStep2}
      \vb
    \item Use cross-validation (or some other suitable method) to choose the 
      best model out of the $K + 1$ candidates selected in Step \ref{bssStep2}
    \end{enumerate}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Best Subset Selection}
  
  BSS will select a reasonably good model, but it is usually computationally 
  infeasible
  \vb
  \begin{itemize}
  \item To estimate all subsets, we must fit $2^K$ models
    \vc
    \begin{itemize}
    \item $K = 10 \Rightarrow$ \Sexpr{2^10} subsets
      \vc
    \item $K = 20 \Rightarrow$ \Sexpr{as.integer(2^20)} subsets
      \vc
    \item $K = 30 \Rightarrow$ \Sexpr{as.integer(2^30)} subsets
    \end{itemize}
    \vc
  \item Even with efficient algorithms that remove some possiblities, BSS is 
    only feasible when $K < 40$
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Stepwise Regression}

  The idea of stepwise regression is to build a good model by iteratively adding 
  or removing the single most influential predictor.
  \vb
  \begin{itemize}
  \item Stepwise regression comes in three basic flavors:
    \vb
    \begin{enumerate}
    \item Forward selection (AKA, Build-up)
      \begin{itemize}
      \item Start with a trivial model and add predictors
      \end{itemize}
      \vb
    \item Backward elimination (AKA, Tear-down)
      \begin{itemize}
      \item Start with the largest possible model and remove predictors
      \end{itemize}
      \vb
    \item Bidirectional elimination (AKA, Full stepwise)
      \begin{itemize}
      \item At each step, test variables for inclusion and exclusion
      \end{itemize}
    \end{enumerate}
  \end{itemize}

  \vb
  
  The entry/exclusion checks at each step can be done by statistical tests 
  (e.g., t-tests) or any reasonable measure of model fit.

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Forward Selection}
  
  Assuming a set of $K$ candidate predictors:
  \vc
  \begin{enumerate}
  \item Estimate the intercept only model, $Y = \beta_0 + \varepsilon$
    \vc
  \item Find the best model (e.g., the model with largest $R^2$) among the $K$ 
    possibilities that include exactly one predictor \label{fsStep2}
    \vc
  \item Find the best model among the $K - 1$ possiblities that add an 
    additional predictor to the model selected in Step \ref{fsStep2} 
    \vc
  \item Continue iterating by adding single predictors to the model selected in 
    the previous step until all predictors are included in the model
    \vc
  \item Use cross-validation (or some other suitable method) to choose the 
    best model out of the final $K + 1$ candidates
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Backward Elimination}
  
  Assuming a set of $K$ candidate predictors:
  \vc
  \begin{enumerate}
  \item Estimate the full least squares model including all $K$ predictors, 
    $Y = \beta_0 + \sum_{k = 1}^K X_k + \varepsilon$ \label{beStep1}
    \vc
  \item Find the best model (e.g., the model with largest $R^2$) among the $K$ 
    possibilities that exclude exactly one predictor from the full model \label{fsStep2}
    \vc
  \item Find the best model among the $K - 1$ possiblities that exclude an 
    additional predictor from the model selected in Step \ref{fsStep2} 
    \vc
  \item Continue iterating by removing single predictors from the model selected 
    in the previous step until all predictors are removed from the model
    \vc
  \item Use cross-validation (or some other suitable method) to choose the 
    best model out of the final $K + 1$ candidates
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{}
  
  \begin{center}
    \Huge{\textsc{Model Regularization}}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Ridge Regression}
  
  Let's revisit the of \emph{ordinary least squares} (OLS) objective function.
  
  \begin{itemize}
  \item Consider models of this form:
    \begin{align*}
      Y = \beta_0 + \sum_{p = 1}^P \beta_p X_p + \varepsilon
    \end{align*}
  \end{itemize}
  
  \pause
  
  The traditional OLS objective function is given by:
  \begin{align*}
    RSS_{ols} = \sum_{n=1}^N \left(Y_n - \hat{\beta}_0 - \sum_{p = 1}^P 
    \hat{\beta}_p X_{np} \right)^2
  \end{align*}
  
  \pause
  
  Ridge regression simply adds the $\ell_2$ norm of the regression coefficients:
  \begin{align*}
    RSS_{ridge} = \sum_{n=1}^N \left(Y_n - \hat{\beta}_0 - \sum_{p = 1}^P 
    \hat{\beta}_p X_{np} \right)^2 {\color{red} + 
      \lambda \sum_{p=1}^P\hat{\beta}_p^2}
  \end{align*}

\end{frame}

%------------------------------------------------------------------------------%
  
\begin{frame}{LASSO}
  
  Ridge regression works well for stabilizing predictions and coefficient 
  estimates.  
  \vb
  \begin{itemize}
  \item All variables stay in the model and are shrunk proportionately
    \vb
  \item Coefficients loose interpretability
  \end{itemize}
  \pause 
  \vb 
  The \emph{least absolute shrinkage and selection operator} (LASSO)
  regularizes and implements automatic variable selection.  
  \vb
  \begin{itemize}
  \item The LASSO objective is derived by adding the $\ell_1$ norm of the
    regression coefficients to to the OLS objective function:
  \end{itemize}
  \begin{align*}
    RSS_{LASSO} = \sum_{n=1}^N \left(Y_n - \hat{\beta}_0 - \sum_{p = 1}^P 
    \hat{\beta}_p X_{np} \right)^2 {\color{red} + \lambda 
      \sum_{p=1}^P\left| \hat{\beta}_p \right|}
  \end{align*} 
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Example Data}
  
  Data gathered by a group at the University of Kansas School of
  Social Welfare to evaluate the efficacy of the \emph{Kansas
    Intensive Permanency Project} (KIPP).
  \va
  \begin{itemize}
  \item KIPP is the Kansas arm of the nationwide \emph{Permanency
    Innovations Initiative} (PII).
    \vb
  \item PII and KIPP are interventions that are designed to increase
    the permanency of foster care placements
  \end{itemize}
  \va
  The KIPP data are relatively large:
  \vb
  \begin{itemize}
    \item Three waves of data
      \vc
    \item $N = 918$ participants
      \vc
    \item $P \approx 220$ variables
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Data Preparation}
  
  Missing data were imputed 100 times
  \vb
  \begin{itemize}
  \item For simplicity, these examples are based on the aggregate of
    all 100 imputed datasets
  \end{itemize}
  \va
  Nuisance variables and all variables directly related to the
  intervention evaluation were excluded.\\
  \va
  The final dataset contains $N = 918$ rows and $P = 140$ columns
  
\end{frame}


\begin{frame}{Analysis Plan}
  
  \textsc{Goal:} We are interested in predicting problem behaviors at the
  final measurement occasion.  
  \vb
  \begin{itemize}
    \item What variables are most strongly related to the problem behaviors?
      \vc
    \item Can we build a predictive model that will give a good sense of a
      new child's rates of problem behaviors?
  \end{itemize}
  \va 
  \pause
  \textsc{Challenge: } Although we have a reasonably large sample,
  the large number of potential predictors will complicate our
  modeling.
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Ridge Example}
 
  First, let's try ridge regression.
  \va
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, cache = TRUE>>=
fileName1 <- "kippCleanGmData-20161116.rds"
fileName2 <- "kippTypeList-20161115.rds"

gmData   <- readRDS(paste0(dataDir, fileName1))
typeList <- readRDS(paste0(dataDir, fileName2))

## Exclude intervention status and ID from the predictor set:
dat1 <- gmData[ , setdiff(colnames(gmData), c("study_id", "binAge"))]

## Manually code the categorical variables:
noms <- intersect(typeList$nominal, colnames(dat1))
ords <- intersect(typeList$ordinal, colnames(dat1))

dat2          <- dat1[ , setdiff(colnames(dat1), noms)]
dat2[ , ords] <- data.frame(lapply(dat2[ , ords], as.numeric)) - 1

form1     <- as.formula(c("~ ", paste0(noms, collapse = " + ")))
dummyNoms <- model.matrix(form1, data = dat1)[ , -1]

dat2 <- data.frame(dat2, dummyNoms)

## Set aside a validation set:
nTrain <- 820
nValid <- nrow(dat2) - nTrain

permRows <- sample(c(1 : nrow(dat2)))

trainData <- dat2[permRows[1 : nTrain], ]
validData <- dat2[permRows[(nTrain + 1) : nrow(dat2)], ]

y <- as.matrix(trainData$ProblemBehav_T3)
X <- as.matrix(trainData[ , -grep("ProblemBehav_T3", colnames(trainData))])

foldVec <- sample(rep(c(1 : 5), each = nrow(trainData) / 5))

lassoOut <- cv.glmnet(y = y, x = X, alpha = 1, foldid = foldVec)
ridgeOut <- cv.glmnet(y = y, x = X, alpha = 0, foldid = foldVec)
olsOut   <- lm(ProblemBehav_T3 ~ ., data = trainData)
@ 

<<echo = FALSE>>=
plot(ridgeOut)
@

\end{column}

\begin{column}{0.5\textwidth}
  
<<echo = FALSE>>=
plot(ridgeOut$glmnet.fit, xvar = "lambda")
abline(v = log(ridgeOut$lambda.min), lty = 2, lwd = 2)
abline(v = log(ridgeOut$lambda.1se), lty = 3, lwd = 2)
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{LASSO Example}
  
  Let's try throwing LASSO at the KIPP data.
  \va
  \begin{columns}
    \begin{column}{0.5\textwidth}
     
<<echo = FALSE>>=
plot(lassoOut)
@ 

\end{column}

\begin{column}{0.5\textwidth}
  
<<echo = FALSE>>=
plot(lassoOut$glmnet.fit, xvar = "lambda")
abline(v = log(lassoOut$lambda.min), lty = 2, lwd = 2)
abline(v = log(lassoOut$lambda.1se), lty = 3, lwd = 2)
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Prediction Accuracy}
  
  Did we get any improvement from the regularization?
  
<<echo = FALSE, results = "asis">>=
newX <- as.matrix(
    validData[ , -grep("ProblemBehav_T3", colnames(validData))]
)
newY <- as.matrix(validData[ , grep("ProblemBehav_T3", colnames(validData))])

lassoYHats <- predict(lassoOut, newx = newX, s = lassoOut$lambda.min)
ridgeYHats <- predict(ridgeOut, newx = newX, s = ridgeOut$lambda.min)

lassoYHats2 <- predict(lassoOut, newx = newX, s = lassoOut$lambda.1se)
ridgeYHats2 <- predict(ridgeOut, newx = newX, s = ridgeOut$lambda.1se)

olsYHats   <- predict(olsOut, newdata = validData)

predErrorEx <- matrix(c(crossprod(newY - lassoYHats) / length(newY),
                        crossprod(newY - ridgeYHats) / length(newY),
                        crossprod(newY - olsYHats) / length(newY),
                        crossprod(newY - lassoYHats2) / length(newY),
                        crossprod(newY - ridgeYHats2) / length(newY),
                        NA),
                      nrow     = 2,
                      byrow    = TRUE,
                      dimnames = list(c("MSE Min", "MSE 1 SE"), 
                                      c("LASSO", "Ridge", "OLS"))
                      )

tab1 <- xtable(predErrorEx)
print(tab1, booktabs = TRUE)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Variables Selected by LASSO}
  
<<echo = FALSE, results = "asis">>=
lassoCoef.1se <- as.matrix(coef(lassoOut, s = "lambda.1se"))
flag.1se      <- lassoCoef.1se != 0
nzCoef.1se    <- lassoCoef.1se[flag.1se, ]

t1 <- xtable(t(as.matrix(nzCoef.1se[1 : 4])))
t2 <- xtable(t(as.matrix(nzCoef.1se[5 : 8])))
t3 <- xtable(t(as.matrix(nzCoef.1se[9 : 11])))
t4 <- xtable(t(as.matrix(nzCoef.1se[12 : 15])))
t5 <- xtable(t(as.matrix(nzCoef.1se[16 : 19])))

print(t1, digits = 4, booktabs = TRUE, scale = 0.7, include.rownames = FALSE)
print(t2, digits = 4, booktabs = TRUE, scale = 0.7, include.rownames = FALSE)
print(t3, digits = 4, booktabs = TRUE, scale = 0.7, include.rownames = FALSE)
print(t4, digits = 4, booktabs = TRUE, scale = 0.7, include.rownames = FALSE)
print(t5, digits = 4, booktabs = TRUE, scale = 0.7, include.rownames = FALSE)
@ 

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Collinearity}
  
  Predictors are expected to be correlated.
  \vb
  \begin{itemize}
  \item If predictors were never correlated, we would not need multiple linear
    regression.
    \vb
  \item Partialling is a consequence of correlated predictors.
  \end{itemize}
  \vb
  Too much correlation, however, is a problem known as \emph{collinearity}.
  \vb
  \begin{itemize}
  \item With collinear predictors, fitted models are very unstable
    \vb
    \begin{itemize}
    \item A small change in the data can produce dramatic changes in the
      regression coefficients.
      \vb
    \item Regression coefficients have large SEs.
      \vb
    \item Predictions will be highly variable.
    \end{itemize}
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Intuition for Collinearity}
  
  Recall the intuitive representation of partialling via Venn diagrams:
  \va
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      \includegraphics[width = 1.1\textwidth]{figures/good_trivariate_venn}
      
    \end{column}
    
    \begin{column}{0.5\textwidth}
      
      \includegraphics[width = 1.1\textwidth]{figures/good_partial_venn}
      
    \end{column}
  \end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Intuition for Collinearity}
  
  With severe collinearity, the intuition looks more like this:
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      \includegraphics[width = 1.1\textwidth]{figures/bad_trivariate_venn}
      
    \end{column}
    
    \begin{column}{0.5\textwidth}
      
      \includegraphics[width = 1.1\textwidth]{figures/bad_partial_venn}
      
    \end{column}
  \end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Consequences of Collinearity}
   
<<"collin_sim", echo = FALSE, cache = TRUE>>=
library(mvtnorm)

r2   <- 0.5
nObs <- 1000

clVec <- seq(0.05, 0.95, 0.1)
nReps <- 500

clList <- list()
for(cl in clVec) {
    rpList <- list()
    for(rp in 1 : nReps) {
        X <- rmvnorm(nObs, c(0, 0), matrix(c(1.0, cl, cl, 1.0), 2, 2))
        
        eta   <- X %*% matrix(c(0.5, 0.75))
        sigma <- (var(eta) / r2) - var(eta)
        
        y <- eta + rnorm(nObs, 0, sqrt(sigma))
        
        dat1 <- data.frame(y, X)
        colnames(dat1) <- c("y", "x1", "x2")
        
        out1 <- lm(y ~ x1 + x2, data = dat1)
        rpList[[rp]] <- coef(out1)
    }
    clList[[as.character(cl)]] <- do.call(rbind, rpList)
}

clMat <- do.call(rbind, clList)

x1Frame <- data.frame(b2 = clMat[ , 2], cl = as.factor(rep(clVec, each = nReps)))
@

\begin{columns}
  \begin{column}{0.5\textwidth}
    
    Draw 500 samples with $N = 1000$ observations from the population defined by 
    the model: 
    \begin{align*}
      &Y = \beta_1 X_1 + \beta_2 X_2 + \varepsilon\\
      &\text{~~with}\\
      &\text{Cor}(X_1, X_2) \in \{0.05, 0.15, \ldots, 0.95\}
    \end{align*}
    \vx{-18}
    \begin{itemize}
    \item Plot the resulting $\hat{\beta}_1$ values.
    \end{itemize}
    
  \end{column}
  
  \begin{column}{0.5\textwidth}
    
<<echo = FALSE>>=
p1 <- gg0(x = x1Frame$cl, y = x1Frame$b2, points = FALSE)
p1 + geom_boxplot() + xlab("Cor(X1, X2)") + ylab("Estimated Beta2")
@

\end{column}
\end{columns}
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Detecting Collinearity}
  
  We can check for bivariate collinearity by examining the correlation matrix of
  the predictors.
  \vb
  \begin{itemize}
  \item Very high correlations between any two predictors should cause concern.
    \vc
    \begin{itemize}
    \item $r > 0.9$ 
    \end{itemize}
  \end{itemize}
  \vb
  Often collinearity does not manifest as a simple bivaraite relation.
  \vb
  \begin{itemize}
  \item With \emph{multicollinearity} one predictor variable is a function of
    two, or more, others.  
    \vc
    \begin{itemize}
    \item Multicollinearity is often undetectable through bivariate correlations.
    \end{itemize}
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Multicollinearity}
 
<<>>=
nObs <- 10000

## Simulate 'x' and 'w' as independent normal:
x <- rnorm(nObs)
w <- rnorm(nObs)

## 'z' is a perfect function of 'x' and 'w':
z <- x + w

## Cor(z, w) and Cor(z, x) are not too big, though:
round(cor(data.frame(w, x, z)), 3)
@ 

\pagebreak

<<size = "tiny">>=
## Generate some outcome data:
y <- 0.5 * x + 0.75 * w + 0.25 * z + rnorm(nObs, 0, 1.5)

## Try to model 'y' with 'x', 'w', and 'z':
out1 <- lm(y ~ x + w + z)
summary(out1)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Consequences of Multicollinearity}
  
<<"mcollin_sim", echo = FALSE, cache = TRUE>>=
## Multicollinearity simulation:
nObs  <- 1000
r2    <- 0.5
clVec <- seq(0.05, 0.95, 0.1)
nReps <- 500

clList <- list()
for(cl in clVec) {
    rpList <- list()
    for(rp in 1 : nReps) {
        X <- rmvnorm(nObs, c(0, 0), matrix(c(1.0, 0, 0, 1.0), 2, 2))
        
        etaZ   <- X %*% matrix(c(1, 1))
        sigmaZ <- (var(etaZ) / cl) - var(etaZ)
        
        z <- etaZ + rnorm(nObs, 0, sqrt(sigmaZ))
        
        XZ <- cbind(X, z)
        
        etaY   <- XZ %*% matrix(c(0.25, 0.5, 0.75))
        sigmaY <- (var(etaY) / r2) - var(etaY)
        
        y <- etaY + rnorm(nObs, 0, sqrt(sigmaY))
        
        dat1           <- data.frame(y, XZ)
        colnames(dat1) <- c("y", "x1", "x2", "z")
        
        out1 <- lm(y ~ x1 + x2 + z, data = dat1)
        rpList[[rp]] <- coef(out1)
    }
    clList[[as.character(cl)]] <- do.call(rbind, rpList)
}

clMat <- do.call(rbind, clList)

zFrame <- data.frame(b3 = clMat[ , 4], cl = as.factor(rep(clVec, each = nReps)))
@ 

\begin{columns}
  \begin{column}{0.5\textwidth}
    
    Draw 500 samples with $N = 1000$ observations from the population defined by 
    the model: 
    \begin{align*}
      &Y = \beta_1 X_1 + \beta_2 X_2 + \beta_3 Z + \varepsilon\\
      &\text{~~with}\\
      &\text{Cor}(X_1, X_2) = 0 \text{~and~} Z = X_1 + X_2 + \theta
    \end{align*}
    \vx{-18}
    \begin{itemize}
    \item Manipulate $\theta$ to induce: $R_{Z.\mathbf{X}}^2 \in \{0.05, 0.15, \ldots, 
      0.95\}$
      \vb
    \item Plot the resulting $\hat{\beta}_3$ values.
    \end{itemize}
    
  \end{column}
  
  \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p1 <- gg0(x = zFrame$cl, y = zFrame$b3, points = FALSE)
p1 + geom_boxplot() + xlab("R Squared Z.{X1,X2}") + ylab("Estimated Beta3")
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{How Ridge Helps with Collinearity}
  
  Let $\mathbf{X} = \{X_1, X_2, \ldots, X_P\}$.
  \vb
  \begin{itemize}
  \item Minimizing the OLS objective function yields fitted regression
    coefficients of the form:
    \begin{align*}
      \hat{\beta}_{ols}=\text{Cov}(\mathbf{X})^{-1} \text{Cov}(\mathbf{X}, Y) 
    \end{align*}
    
  \item With severe multicollinearity, $\text{Cov}(\mathbf{X})^{-1}$ does not
    exist.
    \vb
    \begin{itemize}
    \item The ridge penalty adds a small constant $\lambda$ to the diagonal of
      $\text{Cov}(\mathbf{X})$:
      \begin{align*}
        \hat{\beta}_{ridge}=\text{Cov}(\mathbf{X} {\color{red} + \lambda
          \mathbf{I}_P} )^{-1} \text{Cov}(\mathbf{X}, Y)
      \end{align*}
    \end{itemize}
    
  \item For sufficiently large $\lambda$, $\text{Cov}(\mathbf{X} + \lambda
    \mathbf{I}_P )^{-1}$ \emph{does} exist.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Ridge vs. Collinearity}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      Draw 250 samples with $N = 100$ observations from the population defined
      by the model:
      \begin{align*}
        Y = \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_{25} X_{25} + \varepsilon\\
      \end{align*}
      \vx{-36}
      \begin{itemize}
      \item $\text{Cor}(X_i, X_j) \in \{0.05, 0.15, \ldots, 0.95\},~i \neq j$
        \vb
      \item Analyze the data with OLS and ridge regression.
        \vb
      \item Plot the Test MSE values.
      \end{itemize}
      
    \end{column}
    
    \begin{column}{0.5\textwidth}
      
<<"reg_sim", echo = FALSE, cache = TRUE>>=
library(glmnet)

## Regularization simulation:
clVec  <- seq(0.05, 0.95, 0.1)
nReps  <- 250
nObs   <- 100
nPreds <- 25
r2     <- 0.5

clList <- list()
for(cl in clVec) {
    rpList <- list()
    for(rp in 1 : nReps) {
        sigma       <- matrix(cl, nPreds, nPreds)
        diag(sigma) <- 1.0
        
        X <- rmvnorm(2 * nObs, rep(0, nPreds), sigma)
        
        eta   <- X %*% matrix(runif(nPreds, 0.5, 0.75))
        sigma <- (var(eta) / r2) - var(eta)
        
        y <- eta + rnorm(2 * nObs, 0, sqrt(sigma))
        
        dat1           <- data.frame(y, X)
        colnames(dat1) <- c("y", paste0("x", c(1 : nPreds)))
        
        dat2 <- as.matrix(dat1[1 : nObs, ])
        dat1 <- as.matrix(dat1[-c(1 : nObs), ])
                
        out1 <- lm(y ~ ., data = as.data.frame(dat1))
        out2 <- cv.glmnet(y = dat1[ , 1], x = dat1[ , -1], alpha = 0)
        out3 <- cv.glmnet(y = dat1[ , 1], x = dat1[ , -1], alpha = 1)
        
        yHat1 <- predict(object = out1, newdata = as.data.frame(dat2))
        yHat2 <- predict(object = out2, newx = dat2[ , -1], s = "lambda.min")
        yHat3 <- predict(object = out3, newx = dat2[ , -1], s = "lambda.min")
        
        rpList[[rp]] <- c(
            crossprod(dat2[ , 1] - yHat1) / nObs,
            crossprod(dat2[ , 1] - yHat2) / nObs,
            crossprod(dat2[ , 1] - yHat3) / nObs
        )
    }
    clList[[as.character(cl)]] <- do.call(rbind, rpList)
}

clMat           <- do.call(rbind, clList)
colnames(clMat) <- c("ols", "ridge", "lasso")

outFrame <- data.frame(clMat, cl = as.factor(rep(clVec, each = nReps)))
@ 

<<echo = FALSE>>=
p1 <- gg0(x = outFrame$cl, y = outFrame$ols, points = FALSE)
p1 + geom_boxplot(fill = "red", outlier.colour = "red") +
geom_boxplot(mapping        = aes(x = outFrame$cl, y = outFrame$ridge), 
             fill           = "blue", 
             outlier.colour = "blue") +
xlab("Cor(X)") +
ylab("Test MSE")
@ 

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Looking Ahead}
  
  \begin{itemize}
  \item A new online assignment will be posted on Friday.
    \vb
  \item An exam study guide will be posted this weekend.
    \vb
  \item Next week we will (very briefly and rapidly) cover logistic regression.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\comment{
\begin{frame}[allowframebreaks]{References}
  
  \bibliographystyle{apacite}
  \bibliography{../../../literature/bibtexStuff/statMethRefs.bib}
  
\end{frame}
}

%------------------------------------------------------------------------------%

\end{document}

