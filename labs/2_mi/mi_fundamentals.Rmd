---
title: "Lab 2a: MI Fundamentals"
subtitle: "Missing Data in R"
author: "Kyle M. Lang"
date: "Updated: `r format(Sys.time(), format = '%Y-%m-%d')`"
params:
  answers: true
output: 
   bookdown::html_document2:
    toc: true
    toc_depth: 1
    toc_float: true
    number_sections: true
    css: "../../resources/style.css"
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)

set.seed(235711)

## Define an asis engine that will evaluate inline code within an asis block:
knit_engines$set(asis = function(options) {
  if(options$echo && options$eval) knit_child(text = options$code)
}
)

opts_chunk$set(include = params$answers, 
               echo = params$answer, 
               message = FALSE,
               warning = FALSE,
               fig.align = "center")
```

---

In these lab exercises, you are going to start working with multiple imputation 
(MI). Specifically, you will do some simple MI-based analyses using he **mice** 
package.

---

# Single Imputation

---

## 

**Load the mice package.**

```{r}
library(mice)
```

---

The **mice** package contains several datasets. Once the package is loaded, 
you can access these datasets in your workspace. In this lab, we will work 
with the *nhanes* dataset ([Schafer, 1997][schafer], Table 6.14).

The *nhanes* dataset is a small data set with non-monotone missing values. These 
data comprise 25 observations of four variables: *age group*, *body mass index*,
*hypertension*,  and *cholesterol*.

Unless otherwise specified, all subsequent questions refer to the *nhanes* 
dataset.

---

##

**Use the `mice()` function to impute the missing data using stochastic 
regression imputation.**

Set the random number seed by specifying a value for the `seed` argument.

```{r, cache = TRUE}
imp1 <- mice(nhanes, method = "norm.nob", m = 1, seed = 235711)
```

```{asis}
This code imputes the missing values in the dataset with a single (because 
`m = 1`) stochastic regression imputation. This approach does not account for 
variability in the regression weights, so it is not 'proper' in the sense of 
[Rubin (1987)][rubin]. The variability of the imputed data will be 
underestimated and, consequently, standard errors for statistics estimated from 
these imputed data will be too small.

Although the above is always true, for data with few missing values, the 
additional complexity of MI is not really necessary. The degree of standard error 
deflation produced by stochastic regression imputation is a function of the 
missing data rate increases. So, stochastic regression imputation can still be a 
good (and simple) solution when the missing data rate is low (say, 5% or less 
missing on any given variable).
```

---

##

**Use the `mice::densityplot()` function to check the imputed values.**

```{r}
densityplot(imp1)
```

```{asis}
In the above figure, the blue lines represent kernel density estimates of the 
observed data, and the red lines represent kernel density estimates of the 
imputed data.

Unless the data are MCAR, these lines need not follow each other very closely.
We don't, however, want to see the red and blue lines completely diverging, 
regardless of the missing data mechanism. These results looks good.
```

---

##

**Use the mice::complete() function to create a completed dataset.**

```{r}
(comp1 <- complete(imp1))
```

```{asis}
The `complete()` function replaces the missing values with the imputations 
generated by the `mice()` run. Now, we have a completed dataset to analyze.
```

---

##

**Use the imputed data to regress `bmi` onto `age`.**

```{r}
fit1 <- lm(bmi ~ age, data = comp1)
summary(fit1)
```

---

##

**Check the affect of the random number seed.**

Rerun the imputation that you did above two times

1. Once using the same value for the random number seed
1. Once using a different value for the random number seed

Confirm that you get the same results in the first case and different results in 
the second case.

```{r, cache = TRUE}
imp2 <- mice(nhanes, method = "norm.nob", m = 1, seed = 235711)
imp3 <- mice(nhanes, method = "norm.nob", m = 1, seed = 314159)

all.equal(imp1, imp2)
all.equal(imp1, imp3)

comp1 - complete(imp2)
comp1 - complete(imp3)
```

```{asis}
Clearly, we've verified the expected pattern. Setting the same seed produces the 
same imputations, while setting a different seed produces different results. 
Stochastic forms of imputation use random sampling ("stochastic" means random), 
so the results will be different if we rerun the imputation algorithm. To get 
exactly the same result, we must set a random number seed.

MI is also a form of stochastic imputation, so we need to set a random number 
seed when doing MI (at least, if we want to be able to replicate our results).
```

---

# Multiple Imputation

---

##

**Check the documentation for the `mice()` function.**

```{r, eval = FALSE}
?mice
```

---

## {#defaultMI}

**Use the `mice()` function to multiply impute the *nhanes* data using all
default options.**

```{r, cache = TRUE}
imp <- mice(nhanes)
```

```{asis}
As you can see from the printed output, the algorithm ran for 5 iterations and 
produced 5 imputations for each missing datum. These numbers result from the 
default values for the `maxit` and `m` arguments, respectively. 
```

---

The object returned by `mice()` has the class *mids* (*m*ultiply *i*mputed 
*d*ata*s*et). This object encapsulates all the relevant information `mice()` 
produced when imputing the *nhanes* dataset (e.g., the original data, the 
imputed values, the number of missing values, number of iterations, etc.).

To see what's inside most R objects, we can use the `ls()` function to list the 
contents of the object.

---

##

**Use the `ls()` function to check the contents of the *mids* object you created
in Question \@ref(defaultMI).**

What do you think is stored in the following slots?

- `data`
- `imp`
- `method`
- `m`

```{r}
ls(imp)
```

```{asis}
The original, incomplete, data are stored in the `data` slot.
```

```{r}
imp$data
```

```{asis}
A list of the imputed values is stored in the `imp` slot.
```

```{r}
imp$imp
```

```{asis}
The vector defining the elementary imputation methods used is stored in the 
`method` slot.
```

```{r}
imp$method
```

```{asis}
The number of imputations is stored in the `m` slot.
```

```{r}
imp$m
```

---

Before proceeding any further, we need to check convergence. If the imputation 
model did not converge, or if the imputed values are not reasonable, there is no 
reason to proceed on to the analysis phase.

---

##

**Use the `plot.mids()` function to create trace plots from the imputed data.**

```{r}
plot(imp)
```

```{asis}
These plots show the means and standard deviations of the imputed values for 
each variable across iterations and imputations. Each line represents an 
different imputed dataset, and the x-axis indexes iteration.

If the imputation model has converged, we should see the lines mixing randomly 
with no trends in the latter parts of these trace plots. Although it's difficult
to judge with so few imputations, it looks like there may be some trending in 
these plots. We should probably consider increasing the number of iterations. 

For now, we're just going to proceed as if everything is peachy. Next week, 
we'll see different ways of adjusting the number of iterations. 
```

---

##

**Use the `mice::densityplot()` function to check the distributions of the 
imputed values.**

```{r}
densityplot(imp)
```

```{asis}
These plots have the same layout as the density plots we generated for the 
single stochastic regression imputation. The only difference lies in the number 
of red curves. Now we have multiple red curves, and each one represents a kernel 
density estimate for one of the sets of imputed values.

As before, we want to see the red and blue curves behaving similarly (though not 
identically). The plots for `bmi` and `hyp` look good, but the `chl` plot 
shows some imputations with "sharp" spikes in their densities. Such patterns 
indicate too little variability in the imputed values.

We should probably do something to address the poor `chl` imputations, but we'll 
table that discussion until next week. 
```

---

# Creating More Imputations

---

At various places above, you should have also seen that `mice()` created 5 sets 
of imputations (which is the default). If we want more imputations (which we 
frequently will), we can set an appropriate value for the `m` argument in the 
`mice()` function.

---

## {#mi2}

**Redo the imputation from Question \@ref(defaultMI).**

This time, change the following settings.

1. Create 20 imputations.
1. Set a random number seed.

*TIP*: Check the documentation for `mice()` to see how to request Bayesian 
linear regression.

```{r, cache = TRUE}
imp2 <- mice(nhanes, m = 20, seed = 235711, print = FALSE)
```

```{asis}
The `print = FALSE` options simply suppresses printed iteration information.
```

---

##

**Check the convergence of the imputation model from Question \@ref(mi2) as you 
did before.**

```{r}
plot(imp2)
densityplot(imp2)
```

```{asis}
The trace plots suggest convergence of the imputation models, and the density 
plots suggest plausible imputed values. We're good to go.
```

---

# Output Formats

---

If we want to create a set of stand-alone multiply imputed datasets, we use the
`mice::complete()` function to do so. 

When completing the data with stochastic regression imputation, we only had one 
sensible way to return the result because we only created a single set of 
imputations. MI, on the other hand, creates *m* different sets of imputations, 
so we have several ways to format the completed datasets. We can select between 
these options via the `action` argument in the `mice::complete()` function.

If we specify a single number for the `action` argument, the function will 
return a single completed datasets corresponding to the requested imputation 
number (e.g., the third imputation in the following code).

```{r, include = TRUE, echo = TRUE, cache = TRUE}
imp <- mice(nhanes, m = 3, print = FALSE)

complete(imp, 3) 
```

We can also request all *m* imputed datasets be collapsed into a single block 
of data with three different structures.

- `action = long` row binds the imputed datasets one atop the other.
- `action = broad` column binds the imputed datasets side-by-side.
- `action = repeated` does the same as `broad` but reorders the variables so the 
*m* replicates of each variable form a contiguous block of data.

```{r, include = TRUE, echo = TRUE}
complete(imp, "long")  
complete(imp, "broad")
complete(imp, "repeated")
```

The list format produced by `action = all` is, by far, the most useful. This 
option returns the imputed datasets in an *m*-element list where each element 
contains one completed dataset. Since R prefers to work with lists, you will 
almost certainly want to use the `action = all` format when completing multiply 
imputed datasets for analysis in R.

```{r, include = TRUE, echo = TRUE}
complete(imp, "all")
```

##

**Use the `mice::complete()` function to generate a *list* of multiply imputed 
datasets from the imputations you created in Question \@ref(mi2).**

Save the result as `miList`.

```{r}
miList <- complete(imp2, "all")
```

---

# Analysis and Pooling

---

Use the *mids* object you created in Question \@ref(mi2) to answer the questions 
in this section.

---

After generating the multiply imputed datasets as we did above, the next step in 
an MI-based analysis is the so-called *analysis phase*. In the analysis phase, 
we fit a substantively interesting analysis model to each of the imputed datasets.

When working with **mice**, the simplest way to implement the analysis phase is 
via the `with.mids()` function. For example, the following code regresses `bmi`
onto `chl` using the 3 imputed datasets created above.

```{r, include = TRUE, echo = TRUE}
fit <- with(imp, lm(bmi ~ chl))
fit
```

As you can see, the `fit` object simply contains 3 fitted `lm` objects. Each 
of these objects represents a regression model fit to one of the imputed datasets 
in the `imp` object.  

The `fit` object returned by `with.mids()` has the class *mira* (*m*ultiply 
*i*mputed *r*epeated *a*nalyses).

```{r, include = TRUE, echo = TRUE}
class(fit)
ls(fit)
```

The actual fitted models live in the `analyses` slot of the `mira` object. Each 
of these fitted models will behave according to their individual types (e.g., a 
fitted `lm` object inside a `mira` object will behave as any other `lm` object 
would).

```{r, include = TRUE, echo = TRUE}
library(dplyr)

fit$analyses[[1]] %>% summary()
fit$analyses[[2]] %>% coef()
fit$analyses[[3]] %>% resid()
```

---

## {#analysisPhase}

**Use `with.mids()` to regress `bmi` onto `age` and `hyp`.**

```{r}
fit2 <- with(imp2, lm(bmi ~ age + hyp))
```

---

The final step in an MI-based analysis is the *pooling phase* wherein we 
aggregate the multiple model fits from the analysis phase. If we've implemented 
the analysis phase via the `with.mids()` function, then we can pool using the 
`mice::pool()` function. For example, the following code will pool the regression 
models estimated above.

```{r, include = TRUE, echo = TRUE}
poolFit <- pool(fit)
summary(poolFit)
```

The `poolFit` object has class `mipo` (*m*ultiply *i*mputed *p*ooled *o*bject). 
The `mipo` object contains the pooled regression coefficients and standard 
errors (pooled with the [Rubin, 1987][rubin], rules), as well as some other
useful statistics (e.g., the fraction of information, `fmi`, the proportion of 
the variation attributable to the missing data, `lambda`). 

```{r, include = TRUE, echo = TRUE}
ls(poolFit)
ls(poolFit$pooled)
poolFit$pooled$fmi
poolFit$pooled$lambda
```

---

##

**Use the `mice::pool()` function to pool the estimates you generated in 
Question \@ref(analysisPhase).**

What inferences can you draw from the MI estimates?

```{r}
pool2 <- pool(fit2)
summary(pool2)
```

```{asis}
At the $\alpha = 0.05$ significance level, neither `age` nor `hyp` significantly
predict `bmi`. 
```

---

The workflow demonstrated above applies to many different types of analysis. As
long as the estimation function provides `coef()` and `vcov()` methods, this 
workflow should succeed. In a future practical, we will explore alternative 
workflows that you can use in situations where the `mice()` $\rightarrow$ 
`with.mids()` $\rightarrow$ `pool()` process doesn't work. 

---

# Imputation Model Predictors

---

By default, `mice()` will impute each incomplete variable using some flavor of 
univariate supervised model wherein all other variables on the dataset act as 
predictors. These individual, univariate models are called the *elementary 
imputation models* (EIMs), and we are completely free to adjust their 
specification. One such adjustment that we often want to make is restricting 
which variables a given EIM uses as predictors.

In `mice()` we adjust the right hand side (RHS) of the EIMs via the 
`predictorMatrix` argument.

---

The predictor matrix is stored in the `predictorMatrix` slot of the *mids* 
object. Let's look at a default predictor matrix.

```{r, include = TRUE, echo = TRUE}
imp$predictorMatrix
```

Each row in the predictor matrix is a pattern vector that defines the predictors 
used in the EIM for each variable in the dataset. Variables that are not imputed 
still get a row in the predictor matrix, their rows are just ignored by `mice()`. 
Consequently, the predictor matrix is square. A value of 1 in a matrix entry 
indicates that the column variable is used as a predictor in the EIM for the row 
variable. For example, the 1 at position [2, 1] indicates that variable `age` 
was used in the imputation model for `bmi`. Note that the diagonal is zero 
because a variable is cannot impute its own missing data. 

We are not forced to use this version of the predictor matrix; `mice` gives us 
complete control over the predictor matrix. So, we can choose our own predictors 
for each EIM in our particular problem. This flexibility can be very useful, for 
example, when you have many variables, or when you have clear ideas or prior 
knowledge about relations in the data. 

We can use a dry-run of `mice()` to initialize a default predictor matrix, 
without doing any imputation. We can then modify this matrix to fit our needs. 
The following code implements this initialization.

```{r, include = TRUE, echo = TRUE, cache = TRUE}
ini <- mice(nhanes, maxit = 0)
(pred <- ini$predictorMatrix)
```

The object `pred` now contains the default predictor matrix for the *nhanes* 
data. Altering the predictor matrix and using the updated version for imputation 
is very simple. For example, the following code removes the variable `hyp` from 
the set of predictors in each EIM.

```{r, include = TRUE, echo = TRUE}
pred[ , "hyp"] <- 0
pred
```

---

We use the modified predictor matrix by supplying it to the `predictorMatrix` 
argument in `mice()`.

```{r, include = TRUE, echo = TRUE, cache = TRUE}
imp <- mice(nhanes, predictorMatrix = pred, print = FALSE, seed = 235711)
```

---

## {#predMat}

**Create a custom predictor matrix for the *nhanes* data.**

Specify the EIMs as follows:

- $Y_{bmi} = \beta_0 + \beta_1 X_{hyp} + \beta_2 X_{chl}$
- $Y_{hyp} = \beta_0 + \beta_1 X_{age} + \beta_2 X_{chl}$
- $Y_{chl} = \beta_0 + \beta_1 X_{bmi}$

```{r}
pred1 <- mice(nhanes, maxit = 0)$predictorMatrix

pred1["age", ] <- 0
pred1["bmi", "age"] <- 0
pred1["hyp", "bmi"] <- 0
pred1["chl", c("age", "hyp")] <- 0

pred1
```

---

The `mice::quickpred()` function provides a simple way to construct predictor 
matrices via various selection rules. The following code will select as 
predictors all variables that correlated at least $\rho = 0.30$ with each 
incomplete variable.

```{r, include = TRUE, echo = TRUE, cache = TRUE}
(pred <- quickpred(nhanes, mincor = 0.3))
```

Now, only `age` and `chl` will be used to impute `bmi` and `hyp`, but all other 
variables will still be used to impute `chl`. Note that the `age` row is all 
zeros because `age` is fully observed, so it needs no EIM.

---

##

**Check the documentation for `quickpred()`.**

Take note of the different selection criteria you can apply.

```{r, eval = FALSE}
?quickpred
```

---

## {#qp}

**Use `quickpred()` to create a predictor matrix for the *nhanes* data.**

This predictor matrix should satisfy the following conditions.

- Use `age` as a predictor in every EIM.
- Select the remaining predictors such that they have a minimum correlation of 
$\rho = 0.45$ with the imputation targets.

Which variables are selected as predictors for each EIM?

```{r, cache = TRUE}
(pred <- quickpred(nhanes, include = "age", mincor = 0.45))
```

```{asis}
Age is the only predictor in each of the imputation models.
```

---

## {#qpImp}

**Use the predictor matrix you created in Question \@ref(qp) to impute the *nhanes* data.**

Use default settings except for the following options.

- Create 8 imputations.
- Set the imputation method to `norm`.
- Set a random number seed.

```{r, cache = TRUE}
imp <- mice(nhanes, 
            m = 8, 
            method = "norm", 
            predictorMatrix = pred, 
            seed = 235711,
            print = FALSE)
```

---

##

**Create trace plots and density plots for the *mids* object you created in Question \@ref(qpImp).**

```{r}
plot(imp)
densityplot(imp)
```

---

# Imputation Methods

---

For each column in the dataset, we must specify an imputation method. The 
`method` slot in a *mids* object shows which methods were applied to each column. 
The following code shows the default methods that would be applied if we impute 
the *nhanes* data.

```{r, include = TRUE, echo = TRUE}
mice(nhanes, maxit = 0)$method
```

The `age` variable is completely observed, so it is not imputed. The the empty 
string, `""`, tells `mice()` not to impute the corresponding variable. The other 
variables will all be imputed via `pmm` (*predictive mean matching*), which is 
the default method for numeric data. 

In reality, these data are actually a mix of numeric and categorical data. The 
*nhanes2* dataset contains appropriately typed categorical variables.

```{r, include = TRUE, echo = TRUE}
summary(nhanes2)
str(nhanes2)
```

In *nhanes2*, the `age` variable is a factor comprising three categories, and 
`hyp` is a binary factor. If we run `mice()` without specifying a `method` 
argument, the algorithm will attempt to match the imputation method for each 
column to the column's  type. 

Let's see what happens when we initialize a `mice()` run on the *nhanes2* 
dataset with the default arguments. 

```{r, include = TRUE, echo = TRUE}
init <- mice(nhanes2, maxit = 0)
init$meth
```

The imputation method for `hyp` is now set to `logreg`, which imputes by 
*logistic regression* (the default for binary factors). The `mice()` algorithm 
has recognized the type of the new `hyp` variable and updated the imputation 
method appropriately.

---

We can get an up-to-date overview of the available methods via the `methods()` 
function. 

```{r, include = TRUE, echo = TRUE}
methods(mice)
```

More detail on the methods is available in the *Details* section of the 
documentation for `mice()`, and we can see detailed documentation for each 
method through the help page for the appropriate `mice.impute.METHOD()` function.

```{r, include = TRUE, echo = TRUE, eval = FALSE}
?mice
?mice.impute.lasso.select.logreg
```

---

In Question \@ref(qpImp), you changed the imputation method for all variables by
setting a single value for the `method` argument. We can also change the methods 
on a variable-by-variable basis. The following code will change the imputation 
method for `bmi` to Bayesian linear regression while leaving the remaining 
methods unaffected.

```{r, include = TRUE, echo = TRUE}
(meth <- ini$method)
meth["bmi"] <- "norm"
meth
```

---

To use this custom method vector, we simply supply `meth` to the `method` 
argument when we call `mice()`. 

```{r, include = TRUE, echo = TRUE, cache = TRUE}
imp <- mice(nhanes2, method = meth, print = FALSE)
```

---

## {#meth}

**Create a custom method vector for the *nhanes2* data.**

Define the following methods.

- `age`: None
- `bmi`: Classification and regression trees
- `hyp`: Logistic regression
- `chl`: Predictive mean matching

```{r}
(meth <- c(age = "", bmi = "cart", hyp = "logreg", chl = "pmm"))

## OR

meth <- mice(nhanes2, maxit = 0)$method
meth["bmi"] <- "cart"
meth["hyp"] <- "logreg"

meth
```

```{asis}
Note that both of the above approaches produce the same result. The first 
approach is more direct and probably preferable for smaller datasets. The second 
approach can be more efficient when working with many variables, especially when 
you only need to use a few non-default methods.
```

---

## {#methImp}

**Impute the missing values in the *nhanes2* dataset.**

Set up the imputation run as follows.

- Use the predictor matrix you created in Question \@ref(predMat).
- Use the method vector you created in Question \@ref(meth).
- Create 15 imputations.
- Set a random number seed.

```{r, cache = TRUE}
imp1 <- mice(nhanes2, 
             m = 15, 
             method = meth, 
             predictorMatrix = pred1,
             seed = 235711,
             print = FALSE)
```

---

##

**Create trace plots and density plots for the *mids* object you created in 
Question \@ref(methImp).**

```{r}
plot(imp1)
densityplot(imp1)
```

---

# Adjusting the Iterations

---

By default, `mice()` will use five iterations. Sometimes, these five will be 
enough, but often we will want to run the algorithm longer. We can set the 
number of iterations via the `maxit` argument.

The following code will run the default imputation model for ten iterations.

```{r, include = TRUE, echo = TRUE, cache = TRUE}
imp <- mice(nhanes2, maxit = 10)
plot(imp)
```

Notice that now the x-axis in the trace plots extends to 10 to reflect the higher
number of iterations.

---

One common reason to increase the number of iterations is non-convergence. We'll 
sometimes observe trends when we inspect the trace plots of the imputed items. 
These trends tell us that the algorithm has not yet reached an equilibrium point. 
In such situations, the first thing to try (so long as you don't notice any 
obvious errors in your imputation model specification) is increasing the number 
of iterations. While we could accomplish this goal simply by rerunning `mice()` 
with a higher value for `maxit`, doing so would be very wasteful. We would have 
to recreate all of the existing iterations before generating any new iterations. 
Thankfully, we don't need to be so inefficient.

We can easily extend the chains in any *mids* object with the `mice.mids()` 
function. This function takes as *mids* object as its primary argument and picks 
up the sampling where `mice()` left off (using exactly the same imputation model
specification). The following code will extend the chains in our most recent 
*mids* object to 25 total iterations by adding on another 15 iterations.

```{r, include = TRUE, echo = TRUE, cache = TRUE}
imp <- mice.mids(imp, maxit = 15, print = FALSE)
plot(imp)
```

Now, `imp` contains 25 iterations, which we can see by the range of the x-axis 
in the trace plots.

---

## {#iterImp}

**Increase the number of iterations for the imputation you ran in Question 
\@ref(methImp) to 25.**

```{r, cache = TRUE}
imp1 <- mice.mids(imp1, maxit = 10, print = FALSE)
```

---

# More Diagnostics

---

Recall that even if the imputation model converges (e.g., as demonstrated by 
trace plots), the imputed values still might not be reasonable estimates of the 
missing data. If the data are MCAR, then the imputations should have the same 
distribution as the observed data. In general, though, the distributions of the 
observed and imputed data may differ when the data are MAR. However, we don't 
want to see very large discrepancies.

Strip plots are another excellent visualization tool with which we can assess
the comparability of the imputed and observed data. The `mice::stripplot()` 
function will generate strip plots from a *mids* object. 

The following code will create a strip plot of the observed and imputed `chl` 
values.

```{r, include = TRUE, echo = TRUE}
stripplot(imp, chl)
```

The observed data are plotted in blue, and the imputed data are plotted in red. 
Since `chl` was imputed with PMM (which draws imputations from the observed data), 
the imputed values have the same gaps as the observed data, and the imputed 
values are always within the range of the observed data. This figure indicates 
that the observed and imputed values of `chl` follow similar distributions. So, 
we should conclude that these imputations are plausible.

We can also call `stripplot()` without specifying any variables. Doing so will 
plot all of the imputed variables in an array. 

```{r, include = TRUE, echo = TRUE}
stripplot(imp)
```

---

##

**Create trace plots, density plots, and strip plots for the *mids* object you 
created in Question \@ref(iterImp).**

Based on these visualizations, would you say the imputation model has converged 
to a valid solution?

```{r}
plot(imp1)
densityplot(imp1)
stripplot(imp1)
```

```{asis}
Yes, these results look pretty good. The trace plots show the chains mixing well 
without any trends, and the density and strip plots show no extreme or 
implausible values.
```

---

## {#imp2}

**Rerun the imputation from Question \@ref(iterImp).**

Keep all the same settings, but use Bayesian linear regression to impute `bmi` 
and `chl`.

```{r, cache = TRUE}
meth <- imp1$method
meth[c("bmi", "chl")] <- "norm"
meth

imp2 <- mice(nhanes2, 
             m = 15, 
             method = meth, 
             predictorMatrix = pred1, 
             maxit = 25, 
             seed = 235711, 
             print = FALSE)
```

---

##

**Create trace plots, density plots, and strip plots for the *mids* object you
created in Question \@ref(imp2).**

Based on the visualizations, are the imputations from Question \@ref(iterImp) or 
Question \@ref(imp2) more reasonable? Why?

```{r}
plot(imp2)
densityplot(imp2)
stripplot(imp2)
```

```{asis}
Both sets of imputations seem acceptable and reasonable, but each set also has 
its own potential issues. The donor-based imputations of `bmi` and `chl` from 
Question \@ref(iterImp) show a tendency to cluster around the center of the 
distribution (hence the "sharp" spikes in some of the imputed densities). The 
normal-theory imputations from Question \@ref(imp2), on the other hand, contain 
some very high and very low values for both `bmi` and `chl`.

The choice between these two results probably comes down to which aspects of the 
distribution are more important for your application. If maintaining the same 
range as the observed data is important, then the PMM- and CART-based imputations 
are probably better. If a more accurate representation of variability is 
important (and the out-of-bound values don't matter), then the normal-theory 
imputations may be preferable.
```

---

## {#model}

**Use the multiply imputed data you created in Question \@ref(iterImp) to 
estimate the following regression model.**

$Y_{bmi} = \beta_0 + \beta_1 X_{age} = \beta_2 X_{chl} + \varepsilon$

```{r}
fit <- with(imp1, lm(bmi ~ age + chl))
```

---

##

**Pool the estimates from the model you estimated in Question \@ref(model).**

Is the effect of `chl` on `bmi` significant after controlling for `age`?

```{r}
est <- pool(fit)
summary(est)
```

```{r, include = FALSE}
b <- est$pooled$estimate[4]
se <- sqrt(est$pooled$t)[4]
t <- b / se
df <- est$pooled$df[4]
p <- 2 * pt(t, df = df, lower.tail = FALSE)
``` 


```{asis}
No, cholesterol level does not significantly predict BMI after controlling for 
age ($\beta = `r round(b, 2)`$, $t[`r round(df, 2)`] = `r round(t, 2)`$, 
$p = `r round(p, 3)`$).
```

---

##

**Rerun the regression analysis from above using the imputed data from Question 
\@ref(imp2).**

Do the results differ between the two versions of the analysis? Why or why not?

```{r}
est2 <- with(imp2, lm(bmi ~ age + chl)) %>% pool()
summary(est2)
```

```{asis}
The estimates differ slightly due to the different imputation methods for `bmi` 
and `chl`. Also, these differences were just enough to tip the effect of `chl` 
over into statistical significance at the $\alpha = 0.05$ level.
```


---

End of Lab 2a

---

[schafer]: https://www.routledge.com/Analysis-of-Incomplete-Multivariate-Data/Schafer/p/book/9780412040610
[rubin]: https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316696
