%%% Title:    Missing Data in R: FIML
%%% Author:   Kyle M. Lang
%%% Created:  2015-09-29
%%% Modified: 2023-01-29

\documentclass[10pt]{beamer}
\usetheme{Utrecht}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{fancybox}
\usepackage{booktabs}
\usepackage{eurosym}
\usepackage{caption}
\usepackage[mathcal]{eucal}
\usepackage{upgreek}

\captionsetup{labelformat = empty}

\newcommand{\rmsc}[1]{\textrm{\textsc{#1}}}
\newcommand{\pkg}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\mub}[0]{\boldsymbol{\muup}}

\title{Full Information Maximum Likelihood}
\subtitle{Utrecht University Winter School: Missing Data in R}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{}

%------------------------------------------------------------------------------%

\begin{document}

<<knitr_setup, include = FALSE, cache = FALSE>>=
options(width = 60)
opts_chunk$set(size = "footnotesize",
               fig.align = "center",
               fig.path = "figure/fiml-",
               message = FALSE,
               warning = FALSE,
               comment = "")
opts_knit$set(root.dir = "../../../")
knit_theme$set('edit-kwrite')
@

<<r_setup, include = FALSE, cache = FALSE>>=
set.seed(235711)

library(knitr)
library(ggplot2)
library(mice)
library(mvtnorm)
library(xtable)
library(pROC)
library(dplyr)
library(mgcv)
library(optimx)
library(lavaan)

source("code/supportFunctions.R")
source("code/sim_missing/code/simMissingness.R")

dataDir <- "data/"
@

%------------------------------------------------------------------------------%

\begin{frame}[t, plain]
  \titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{FIML Intuition}

  FIML is an ML estimation method that is robust to ignorable nonresponse.
  \vc
  \begin{itemize}
  \item FIML partitions the missing information out of the likelihood function 
    so that the model is only estimated from the observed parts of the data.
  \end{itemize}
  \vb
  After a minor alteration to the likelihood function, FIML reduces to simple ML 
  estimation.
  \vc
  \begin{itemize}
  \item So, let's review ML estimation before moving forward.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\section{Maximum Likelihood}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Maximum Likelihood Estimation}

  ML estimation simply finds the parameter values that are ``most likely'' to 
  have given rise to the observed data.
  \vb
  \begin{itemize}
  \item The \emph{likelihood} function is just a probability density (or mass) 
    function with the data treated as fixed and the parameters treated as 
    random variables.
    \vb
  \item Having such a framework allows us to ask: ``Given that I've observed 
    these data values, what parameter values most probably describe these 
    data?''
  \end{itemize}
  
  \pagebreak
  
  ML estimation is usually employed when there is no closed form solution for 
  the parameters we seek.
  \vb
  \begin{itemize}
  \item This is why you don't usually see ML used to fit general linear models.
  \end{itemize}
  \vb
  After choosing a likelihood function, we iteratively optimize the function to 
  produce the ML estimated parameters.
  \vb
  \begin{itemize}
  \item In practice, we nearly always work with the natural logarithm of the 
    likelihood function (i.e., the \emph{loglikelihood}).
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{ML Intuition}
  
  Let's say we have the following $N = 10$ observations.
  \vc
  \begin{itemize}
  \item We assume these data come from a normal distribution with a known 
    variance of $\sigma^2 = 1$.  
    \vc
  \item We want to estimate the mean of this distribution, $\mu$.
  \end{itemize}
  
  <<echo = 2>>=
  options(width = 50)
  (y <- rnorm(n = 10, mean = 5, sd = 1))
  options(width = 80)
  @
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{ML Intuition}
  
  In ML estimation, we would define different normal distributions.
  \vc
  \begin{itemize}
  \item Every distribution would have $\sigma^2 = 1$.
    \vc
  \item Each distribution would have a different value of $\mu$.
  \end{itemize}
  \vb
  We then compare the observed data to those distributions and see which 
  distribution best fits the data.

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{ML Intuition}
  
  <<echo = FALSE, out.width = "65%">>=
  x1 <- seq(0, 10, length.out = 1000)
  y1 <- dnorm(x1, 3, 1)
  y2 <- dnorm(x1, 4, 1)
  y3 <- dnorm(x1, 5, 1)
  y4 <- dnorm(x1, 6, 1)
  
  dat1 <- data.frame(m = rep(c(3, 4, 5, 6), each = length(x1)),
                     x = rep(x1, 4), 
                     y = c(y1, y2, y3, y4)
                     )
  
  dat2 <- data.frame(m = rep(c(3, 4, 5, 6), each = length(y)),
                     x = rep(y, 4),
                     y = c(dnorm(y, 3, 1),
                           dnorm(y, 4, 1),
                           dnorm(y, 5, 1),
                           dnorm(y, 6, 1)
                           )
                     )
  
  ggplot(data = dat1, mapping = aes(y = y, x = x)) + 
      geom_line() + 
      geom_segment(data = dat2,
                   mapping = aes(x = x, xend = x, y = 0, yend = y),
                   color = "red") +
      theme_classic() +
      ylab("Density") +
      xlab("Data Values") +
      facet_wrap(vars(m))
  @
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Likelihoods}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      Suppose we have the following model:
      \begin{align*}
        Y \sim \text{N}\left( \mu, \sigma^2 \right).
      \end{align*}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
      <<echo = FALSE, cache = TRUE>>=
      x    <- seq(0, 15, length.out = 1000)
      dat1 <- data.frame(x = x, y = dnorm(x, 7.5, 1.75))
      
      ggplot(data = dat1, aes(x = x, y = y)) + 
          theme_classic() +
          geom_line() +
          theme(text       = element_text(size = 16, family = "Courier"),
                plot.title = element_text(size = 20, face = "bold", hjust = 0.5)
                ) +
          ylab("Density") +
          xlab("Y")
      @

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Likelihoods}
  
  For a given $Y_n$, we have:
  \begin{align}
    P \left( Y_n|\mu, \sigma^2 \right) = 
    \frac{1}{\sqrt{2 \pi \sigma^2}} 
    e^{-\frac{\left( Y_n - \mu \right)^2}{2\sigma^2}}. \label{margPdf}
  \end{align}
  
  If we plug estimated parameters into Equation \ref{margPdf}, we get the 
  probability of observing $Y_n$ given $\hat{\mu}$ and $\hat{\sigma}^2$:
  \begin{align}
    P \left( Y_n|\hat{\mu}, \hat{\sigma}^2 \right) = 
    \frac{1}{\sqrt{2 \pi \hat{\sigma}^2}} 
    e^{-\frac{\left( Y_n - \hat{\mu}\right)^2}{2\hat{\sigma}^2}}. \label{estMargPdf}
  \end{align}
  
  Applying Equation \ref{estMargPdf} to all $N$ observations and multiplying the 
  results produces a \emph{likelihood}:
  \begin{align*}
    \hat{L} \left( \hat{\mu}, \hat{\sigma}^2 \right) = 
    \prod_{n = 1}^N P \left( Y_n|\hat{\mu}, \hat{\sigma}^2 \right).
  \end{align*}
  
\end{frame}

%%----------------------------------------------------------------------------%%

\begin{frame}{Likelihoods}
  
  We generally want to work with the natural logarithm of Equation 
  \ref{estMargPdf}. Doing so gives the \emph{loglikelihood}:
  \begin{align*}
  \hat{\mathcal{L}} \left( \hat{\mu}, \hat{\sigma}^2 \right) &= 
    \ln \prod_{n = 1}^N P \left( Y_n|\hat{\mu}, \hat{\sigma}^2 \right)\\ 
    &= -\frac{N}{2} \ln 2\pi - N \ln \hat{\sigma} - \frac{1}{2\hat{\sigma}^2} 
    \sum_{n = 1}^N \left( Y_n - \hat{\mu} \right)^2
  \end{align*}
  
  ML tries to find the values of $\hat{\mu}$ and $\hat{\sigma}^2$ that maximize 
  $\hat{\mathcal{L}} \left( \hat{\mu}, \hat{\sigma}^2 \right)$.
  \vc
  \begin{itemize}
  \item Find the values of $\hat{\mu}$ and $\hat{\sigma}^2$ that are \emph{most 
    likely}, given the observed values of $Y$.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Likelihoods}
  
    \begin{columns}
    \begin{column}{0.5\textwidth}
      
      Suppose we have a linear regression model:
      \begin{align*}
        Y &= \beta_0 + \beta_1 X + \varepsilon,\\[6pt]
        \varepsilon &\sim \text{N}\left( 0, \sigma^2 \right).
      \end{align*}
      This model can be equivalently written as:
      \begin{align*}
        Y \sim \text{N} \left( \beta_0 + \beta_1 X, \sigma^2 \right)
      \end{align*}
      
    \end{column}
    \begin{column}{0.5\textwidth}

      \begin{figure}
        \includegraphics[width = \textwidth]{figures/conditional_density_figure.png}\\
        \va
        \tiny{Image retrieved from:
          \url{http://www.seaturtle.org/mtn/archives/mtn122/mtn122p1.shtml}}
      \end{figure}
      
    \end{column}
    \end{columns}
    
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Likelihoods}
  
  For a given $\{Y_n, X_n\}$, we have:
  \begin{align}
    P \left( Y_n|X_n, \beta_0, \beta_1, \sigma^2 \right) = 
    \frac{1}{\sqrt{2 \pi \sigma^2}} 
    e^{-\frac{\left( Y_n - \beta_0 - \beta_1 X_n \right)^2}{2\sigma^2}}. \label{olsPdf}
  \end{align}
  
  If we plug our estimated parameters into Equation \ref{olsPdf}, we get the 
  probability of observing $Y_n$ given $\hat{Y}_n = \hat{\beta}_0 + 
  \hat{\beta}_1X_n$ and $\hat{\sigma}^2$.
  \begin{align}
    P \left( Y_n|X_n, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}^2 \right) = 
    \frac{1}{\sqrt{2 \pi \hat{\sigma}^2}} 
    e^{-\frac{\left( Y_n - \hat{\beta}_0 - \hat{\beta}_1 X_n \right)^2}{2\hat{\sigma}^2}} \label{estOlsPdf}
  \end{align}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Likelihoods}
 
  So, our final loglikelihood function would be the following:
  \begin{align*}
  \hat{\mathcal{L}} \left( \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}^2 \right) &= 
    \ln \prod_{n = 1}^N P \left( Y_n|X_n, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}^2 \right)\\ 
    &= -\frac{N}{2} \ln 2\pi - N \ln \hat{\sigma} - \frac{1}{2\hat{\sigma}^2} 
    \sum_{n = 1}^N \left( Y_n - \hat{\beta}_0 - \hat{\beta}_1 X_n \right)^2.
  \end{align*}
  
\end{frame}
  
\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}
    
  <<echo = FALSE>>=
  diabetes <- readRDS(paste0(dataDir, "diabetes.rds"))
  @ 

  <<>>=
  ## Fit a model:
  out1 <- lm(ldl ~ bp + glu + bmi, data = diabetes)
  
  ## Extract the predicted values and estimated residual standard error:
  yHat <- predict(out1)
  s    <- summary(out1)$sigma
  
  ## Compute the row-wise probabilities:
  pY <- dnorm(diabetes$ldl, mean = yHat, sd = s)
  
  ## Compute the loglikelihood, and compare to R's version:
  sum(log(pY)); logLik(out1)[1]
  @

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Multivariate Normal Distribution}
  
  The PDF for the multivariate normal distribution is:
  \begin{align*}
    P(\mathbf{Y}|\mub, \Sigma) = 
    \frac{1}{\sqrt{(2\pi)^P|\Sigma|}} e^{-\frac{1}{2}(\mathbf{Y} - \mub)^T\Sigma^{-1}(\mathbf{Y} - \mub)}.
  \end{align*}
  So, the multivariate normal loglikelihood is:
  \begin{align*}
    \mathcal{L} \left( \mub, \Sigma \right) = 
    -\left[\frac{P}{2} \ln(2\pi) + \frac{1}{2} \ln |\Sigma| + \frac{1}{2} \right] \sum_{n = 1}^N(\mathbf{Y}_n - \mub)^T \Sigma^{-1}(\mathbf{Y}_n - \mub).
  \end{align*}
  Which can be further simplified if we multiply through by -2:
  \begin{align*}
    -2\mathcal{L} \left( \mub, \Sigma \right) = 
    \left[P \ln(2\pi) + \ln |\Sigma| \right] \sum_{n = 1}^N(\mathbf{Y}_n - \mub)^T \Sigma^{-1}(\mathbf{Y}_n - \mub).
  \end{align*}
  
\end{frame}

%%----------------------------------------------------------------------------%%

\begin{frame}{Steps of ML}

  \begin{enumerate}
  \item Choose a probability distribution, $f(Y|\theta)$, to describe the 
    distribution of the data, $Y$, given the parameters, $\theta$.
    \vc
  \item Choose some estimate of $\theta$, $\hat{\theta}^{(i)}$.
    \vc
  \item Compute each row's contribution to the loglikelihood function by 
    evaluating: $\ln \left[f\left(Y_n|\hat{\theta}^{(i)}\right)\right]$. 
    \label{rowContrib}
    \vc
  \item Sum the individual loglikelihood contributions from Step 
    \ref{rowContrib} to find the loglikelihood value, $\hat{\mathcal{L}}$. 
    \label{getLL}
    \vc
  \item Choose a ``better'' estimate of the parameters, $\hat{\theta}^{(i + 1)}$, 
    and repeat Steps \ref{rowContrib} and \ref{getLL}. \label{updateTheta}
    \vc
  \item Repeat Steps \ref{rowContrib} -- \ref{updateTheta} until the change 
    between $LL^{(i - 1)}$ and $LL^{(i)}$ falls below some trivially small 
    threshold.
    \vc
  \item Take $\hat{\theta}^{(i)}$ as your estimated parameters.
  \end{enumerate}

\end{frame}


\watermarkon %-----------------------------------------------------------------%

\sectionslide{Full Information Maximum Likelihood}

%------------------------------------------------------------------------------%

\begin{frame}{From ML to FIML}
  
  The $n$th observation's contribution to the multivariate normal loglikelihood 
  function would be the following:
  \begin{align}
    \mathcal{L} \left( \mub, \Sigma \right)_n = 
    -\frac{P}{2} \ln(2\pi) - \frac{1}{2} \ln |\Sigma| - \frac{1}{2} (\mathbf{Y}_n - \mub)^T \Sigma^{-1}(\mathbf{Y}_n - \mub). \label{llContrib}
  \end{align}\\
  \va
  \pause
  FIML just tweaks Equation \ref{llContrib} a tiny bit: 
  \begin{align*}
    \mathcal{L} \left( \mub, \Sigma \right)_{fiml,n} = 
    -\frac{P}{2} \ln(2\pi) - \frac{1}{2} \ln |\Sigma_q| - \frac{1}{2} (\mathbf{Y}_n - \mub_q)^T \Sigma_q^{-1}(\mathbf{Y}_n - \mub_q).
  \end{align*}
  Where $q = 1, 2, \ldots, Q$ indexes response patterns.
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Visualize the Response Patterns}
  
  <<echo = FALSE>>=
  ## Impose MAR missing:
  dat2 <- diabetes %>%
      select(bmi, ldl, glu) %>%
      imposeMissData(targets = c("ldl", "glu"),
                     preds   = "bmi",
                     pm      = 0.3,
                     types   = c("low", "high"),
                     stdDat  = TRUE)
  
  nPats <- md.pattern(dat2, plot = FALSE) %>% nrow() - 1
  @
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      These data contain \Sexpr{nPats} unique response patterns.
      \vc
      \begin{itemize}
      \item We'd define \Sexpr{nPats} different version of $\mu$ and $\Sigma$.
        \vc
      \item We'd calculate each individual loglikelihood contributions using 
        the appropriate flavor of $\mu$ and $\Sigma$. 
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
      <<echo = FALSE>>=
      ggmice::plot_pattern(dat2)
      @
      
    \end{column}
  \end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{FIML Example}

  We most often apply FIML when estimating latent variable models.
  \vc
  \begin{itemize}
  \item We can eastily apply FIML to latent variable models in \pkg{lavaan}.
  \end{itemize}
  
  <<>>=
  library(lavaan)
  
  ## Read in some data:
  bfi <- readRDS(paste0(dataDir, "bfi_datasets.rds"))$incomplete
 
  ## Specify the measurement model:
  cfaMod <- '
  agree =~ A1 + A2 + A3 + A4 + A5
  open  =~ O1 + O2 + O3 + O4 + O5
  '
  
  ## Estimate the CFA using FIML:
  fimlOut <- cfa(cfaMod, data = bfi, std.lv = TRUE, missing = "fiml")
  @ 
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{FIML Example}
  
  <<echo = FALSE>>=
  options(width = 80)
  @ 
  
  <<>>=
  partSummary(fimlOut, 1:4)
  @ 
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{FIML Example}
  
  <<>>=
  partSummary(fimlOut, 7, fmi = TRUE)
  @ 
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{FIML Example}
  
  <<>>=
  partSummary(fimlOut, 8, fmi = TRUE)
  @ 
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{FIML Example}
  
  <<>>=
  partSummary(fimlOut, 9, fmi = TRUE)
  @ 
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{FIML Example}
  
  <<>>=
  partSummary(fimlOut, 10, fmi = TRUE)
  @ 

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{FMI with FIML}

  As you saw above, we can also estimate the FMI when using FIML.
  \vc
  \begin{itemize}
  \item The FMI is calculated using the method described by 
    \citet{savaleiRhemtulla:2012}.
  \end{itemize}
  \vb
  \citet{savaleiRhemtulla:2012} take an information-theoretic approach to 
  defining the FMI.
  \begin{itemize}
  \item Based on the \emph{Missing Information Principle} of 
    \citet{orchardWoodbury:1972}
    \vc  
  \item Their FMI estimates the ratio of missing to complete information for 
    each parameter.
  \end{itemize}
  \vb
  You can use this method to compute the FMI for sufficient statistics via the 
  \pkg{semTools}::\code{fmi()} function.
    
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\sectionslide{Auxiliary Variables}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Satisfying the MAR Assumption}
 
  Like MI, FIML also requires MAR data.
  \vc
  \begin{itemize}
  \item Parameters will be biased when data are MNAR.
  \end{itemize}
  \vb
  Unlike MI, FIML directly treats the missing data while estimating the analysis 
  model.
  \vc
  \begin{itemize}
  \item The MAR predictors must be included in the analysis model.
    \vc
  \item Otherwise, FIML reduces to pairwise deletion.
  \end{itemize}
  \vb  
  When the MAR predictors are not substantively interesting variables, naively 
  included them in the analysis model can change the model's meaning.
 
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Saturated Correlates Technique}

  \citet{graham:2003} developed the \emph{saturated correlates} approach to 
  meet two desiderata:
  \vc
  \begin{enumerate}
  \item Satisfy the MAR assumption by incorporating MAR predictors into the 
    analysis model.
    \vc
  \item Maintain the fit and substantive meaning of the analysis model.
  \end{enumerate}
  \vb
  The approach entails incorporating the MAR predictors via a fully-saturated 
  covariance structure:
  \vc
  \begin{enumerate}
  \item Allow every MAR predictor to covary with all other MAR predictors.
    \vc
  \item Allow every MAR predictor to covary with all observed variables in the
    analysis model (or their residuals).
  \end{enumerate}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Saturated Correlates Diagram}
  
  \begin{figure}
    \includegraphics[width = 0.65\textwidth]{figures/saturated_correlates_diagram.pdf}
  \end{figure}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Saturated Correlates Example}

  We can use the \code{lavaan.auxiliary()} function from \pkg{semTools} (or one 
  of its wrappers) to streamline the analysis.

  <<cache = TRUE>>=
  library(semTools)

  ## Estimate the CFA from above with auxiliary variables:
  fimlOut2 <- bfi %>%
      mutate(male = as.numeric(sex == "male")) %>%
      cfa.auxiliary(cfaMod, 
                    data = ., 
                    aux = c("age", "male"), 
                    std.lv = TRUE)
  @ 
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Saturated Correlates Example}

  The \code{cfa.auxiliary()} function has automatically added the following
  paths to our CFA model.
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      <<echo = FALSE>>=
      modString <- lavExport(fimlOut2, export = FALSE) %>%
          cat() %>%
          capture.output()
      
      grep("age ~~", modString, value = TRUE) %>% cat(sep = "\n")
      @ 
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
      <<echo = FALSE>>=
      grep("male ~~", modString, value = TRUE) %>% cat(sep = "\n")
      @ 

    \end{column}
  \end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Saturated Correlates Example}

  The auxiliaries have been correlated with all other variables.

  <<>>=
  inspect(fimlOut2, "est")$theta[11:12, 1:6] %>% round(3)
  inspect(fimlOut2, "est")$theta[11:12, 7:12] %>% round(3)
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Saturated Correlates Example}

  The degrees of freedom have not changed, though.
  
  <<>>=
  ## Naive FIML:
  fitMeasures(fimlOut, "df")
  
  ## FIML w/ saturated correlates:
  fitMeasures(fimlOut2, "df")
  @
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Saturated Correlates Example}

  Let's compare the effects of the various missing data treatments on the latent
  covariance estimates.
  
  \va
  
  <<echo = FALSE, results = "asis">>=
  obj <- readRDS(paste0(dataDir, "lavaan_objects.rds"))
  
  what <- "agree~~open"
  
  est <- with(obj,
              c(coef(compOut)[what],
                coef(naiveOut)[what],
                coef(miOut)[what],
                coef(fimlOut)[what],
                coef(fimlOut2)[what])
              )
  
  fmi <- with(obj,
              c(getFmi(miOut, what), 
                getFmi(fimlOut, what), 
                getFmi(fimlOut2, what)
                )
              )
  
  name1 <- c("Complete", "Listwise", "Multiple", "Naive", "FIML w/")
  name2 <- c("Data", "Deletion", "Imputation", "FIML", "Auxiliaries")
  name3 <- c("", "", "Est", "FMI")
  
  tabData <- cbind(name3, 
                   rbind(name1, 
                         name2, 
                         sprintf("%.3f", est), 
                         c("---", "---", sprintf("%.3f", fmi))
                         )
                   )
  
  xtable(tabData, caption = "Latent Covariances", align = "lrrrrrr") %>%
      print(hline.after = c(-1, 2, nrow(tabData)), 
            include.colnames = FALSE,
            include.rownames = FALSE,
            booktab = TRUE)
  @
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[allowframebreaks]{References}
  
  \bibliographystyle{apacite}
  \bibliography{../../../bibtex/missing_data_papers.bib}
  
\end{frame}

%------------------------------------------------------------------------------%

\end{document}
