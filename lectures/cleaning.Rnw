%%% Title:    DSS Stats & Methods: Lecture 3
%%% Author:   Kyle M. Lang
%%% Created:  2015-11-06
%%% Modified: 2020-08-26

\documentclass{beamer}
\usetheme[%
  pageofpages          = of,
  bullet               = circle,
  titleline            = true,
  alternativetitlepage = true,
  titlepagelogo        = Logo3,
  watermark            = watermarkTiU,
  watermarkheight      = 100px,
  watermarkheightmult  = 4%
]{UVT}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{booktabs}
\usepackage{eurosym}
\usepackage{fancybox}
\usepackage{caption}

%% Ensure styles of `blocks' (used in Definitions, Theorems etc.) follows the
%% UVT-style theme:
\setbeamercolor{block title}{fg = darkblue, bg = white}
\setbeamercolor{block body}{use = block title, bg = block title.bg}

%% Ensure TableOfContents is in UVT-style theme:
\setbeamercolor{section in toc}{fg = darkblue}

\title{Data Cleaning: Missing Data \& Outliers}
\subtitle{Statistics \& Methodology Lecture 3}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Tilburg University}
\date{}

\newcommand{\R}{\textsf{R}}

%% The following command was adapted from LaTeX Community user 'localghost':
\newcommand*\bigbar[1]{%
  \hbox{%
    \vbox{%
      \hrule height 0.65pt % The actual bar
      \kern0.35ex%         % Distance between bar and symbol
      \hbox{%
        \kern-0.1em%      % Shortening on the left side
        \ensuremath{#1}%
        %\kern-0.1em%      % Shortening on the right side
      }%
    }%
  }%
} 

\begin{document}

<<setup, include = FALSE>>=
set.seed(235711)

library(knitr)
library(ggplot2)
library(plyr)
library(mvtnorm)
library(mice)
library(xtable)
library(carData)
library(SURF)

opts_chunk$set(size = 'footnotesize', fig.align = 'center')
knit_theme$set('edit-kwrite')

dataDir <- "../data/"

lightBlue <- rgb(0, 137, 191, max = 255)
midBlue   <- rgb(0, 131, 183, max = 255)
darkBlue  <- rgb(0, 128, 179, max = 255)
deepGold  <- rgb(184, 138, 45, max = 255)
lightGold <- rgb(195, 146, 48, max = 255)
@

%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
  
  \titlepage
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  
  \begin{itemize}
  \item Data Cleaning
    \vb
    \begin{itemize}
    \item Missing Data Analysis
      \vb
    \item Outlier Analysis
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Data Cleaning}
  
  When we receive new data, they are generally messy and contaminated by various 
  anomalies and errors.
  \vb
  \begin{itemize}
  \item One of the first steps in processing a new set of data is 
    \emph{cleaning}.
    \vc
  \item By cleaning the data, we ensure a few properties:
    \vc
    \begin{itemize}
    \item The data are in an analyzable format.
      \vc
    \item All data take legal values.
      \vc
    \item Any outliers are located and treated.
      \vc
    \item Any missing data are located and treated.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Data Cleaning vis-a-vis the Data Science Cycle}
  
  According to the last lecture, data cleaning is a step that occurs between 
  \emph{data processing} and \emph{EDA}.
  \vb
  \begin{itemize}
  \item In practice, these three steps have nebulous boundaries.
    \vc
  \item In particular, data cleaning and EDA often go hand-in-hand.
    \vc
    \begin{itemize}
    \item Even after cleaning our data, the EDA may expose remaining issues.
      \vc
    \item We will usually iterate between a few rounds of data cleaning and EDA.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%
\begin{frame}{}
  
  \begin{center}
    \Huge{\textsc{Missing Data}}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{A Little Notation}
  
  \begin{align*}
    Y &\coloneqq \text{An $N \times P$ data matrix}\\
    \\
    Y_{mis} &\coloneqq \text{The \emph{missing} part of $Y$}\\
    \\
    Y_{obs} &\coloneqq \text{The \emph{observed} part of $Y$}\\
    \\
    R &\coloneqq \text{An $N \times P$ pattern matrix encoding nonresponse}
  \end{align*}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{What are Missing Data?}
  
  Missing data are empty cells in a dataset where there should be observed 
  values.
  \vc
  \begin{itemize}
  \item The missing cells correspond to true population values, but we haven't 
    observed those values.
  \end{itemize}
  \vb 
  \pause
  Not every empty cell is a missing datum.
  \vc
  \begin{itemize}
  \item Quality-of-life ratings for dead patients in a mortality study
    \vc
  \item Firm profitability after the company goes out of business
    \vc
  \item Self-reported severity of menstrual cramping for men
    \vc
  \item Empty blocks of data following ``gateway'' items
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%
    
\begin{frame}
  
  \begin{center}
    \huge{Missing Data Descriptives}
  \end{center}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\captionsetup{labelformat = empty}

\begin{frame}{Missing Data Pattern}
  
<<echo = FALSE>>=
tmpTab <- matrix(c("x", "x", ".", ".",
                   "y", ".", "y", "."),
                 ncol = 2,
                 dimnames = list(NULL, c("X", "Y"))
                 )

patTab1 <- xtable(tmpTab, align = rep("c", 3), caption = "Patterns for $P = 2$")

tmpTab <- matrix(c(rep("x", 3), ".", "x", rep(".", 3),
                   "y", "y", ".", "y", ".", ".", "y", ".",
                   "z", ".", "z", "z", ".", "z", ".", "."),
                 ncol = 3,
                 dimnames = list(NULL, c("X", "Y", "Z"))
                 )

patTab2 <- xtable(tmpTab, align = rep("c", 4), caption = "Patterns for $P = 3$")
@ 

Missing data (or response) patterns represent unique combinations of observed
and missing items.
\begin{itemize}
  \item $P$ items $\Rightarrow$ $2^P$ possible patterns.
\end{itemize}

\begin{columns}
  \begin{column}{0.45\textwidth}
    
<<echo = FALSE, results = 'asis'>>=
print(patTab1, booktabs = TRUE)
@ 
    
  \end{column}
  \begin{column}{0.45\textwidth}
\vx{-12}    
<<echo = FALSE, results = 'asis'>>=
print(patTab2, booktabs = TRUE)
@ 
     
    \end{column}
  \end{columns}
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Missing Data Pattern}
  
  The concept of a ``missing data pattern'' can also be used to classify the 
  spatial arrangement of missing cells on a data set.\\
      
  \vc
  
  \begin{itemize}
  \item Univariate
    \begin{itemize}
      \item Missing data occur on only one variable.
    \end{itemize}
  
    \vc
  
  \item Monotone
    \begin{itemize}
    \item The proportion of complete elements, in both rows and columns, 
      decreases when traversing the data set.
    \item The observed cells can be arranged into a ``staircase'' pattern.
    \end{itemize}
  
    \vc
  
  \item Arbitrary
    \begin{itemize}
    \item Missing values are ``randomly'' scattered throughout the data set.
    \end{itemize}
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Example Missing Data Patterns}
  
<<echo = FALSE>>=
tmpTab <- matrix(c(rep("x", 10), 
                   rep("y", 5), rep(".", 5),
                   rep("z", 10)), 
                 ncol = 3)
colnames(tmpTab) <- c("X", "Y", "Z")

patTab3 <- xtable(tmpTab, 
                  align   = rep("c", 4),
                  caption = "Univariate Pattern")

tmpTab <- matrix(c(rep("x", 9), rep(".", 1), 
                   rep("y", 6), rep(".", 4),
                   rep("z", 3), rep(".", 7)), 
                 ncol = 3)
colnames(tmpTab) <- c("X", "Y", "Z")

patTab4 <- xtable(tmpTab, 
                  align   = rep("c", 4),
                  caption = "Monotone Pattern")

tmpTab <- matrix(c(rep("x", 10), 
                   rep("y", 10),
                   rep("z", 10)), 
                 ncol = 3)
rMat <- matrix(as.logical(rbinom(30, 1, 0.3)), ncol = 3)
tmpTab[rMat] <- "."
colnames(tmpTab) <- c("X", "Y", "Z")

patTab5 <- xtable(tmpTab, 
                  align   = rep("c", 4),
                  caption = "Arbitrary Pattern")
@ 

\begin{columns}[T]
  \begin{column}{0.33\textwidth}
    
<<echo = FALSE, results = 'asis'>>=
print(patTab3, booktabs = TRUE)
@ 

\end{column}
\begin{column}{0.33\textwidth}
  
<<echo = FALSE, results = 'asis'>>=
print(patTab4, booktabs = TRUE)
@
  
\end{column}
\begin{column}{0.33\textwidth}
  
<<echo = FALSE, results = 'asis'>>=
print(patTab5, booktabs = TRUE)
@ 
  
\end{column}
\end{columns}

\end{frame}

\captionsetup{labelformat = default}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Nonresponse Rates}
  
  \textsc{Percent/Proportion Missing}
  \begin{itemize}
  \item The proportion of cells containing missing data
  \item Good early screening measure
  \item Should be computed for each variable, not for the entire dataset
  \end{itemize}
  
  \va
  
  \textsc{Attrition Rate}
  \begin{itemize}
  \item The proportion of participants that drop-out of a study at each 
    measurement occasion
  \end{itemize}
  
  \va
  
  \textsc{Percent/Proportion of Complete Cases}
  \begin{itemize}
  \item The proportion of observations with no missing data
  \item Often reported but nearly useless quantity
  \end{itemize}
  
  \pagebreak
  
  \textsc{Covariance Coverage}
  \begin{itemize}
  \item The proportion of cases available to estimate a given pairwise
    relationship (e.g., a covariance between two variables)
  \item Very important to have adequate coverage for the parameters you want to 
    estimate
  \end{itemize}
  
  \va
  
  \textsc{Fraction of Missing Information}
  \begin{itemize}
  \item Associated with an estimated parameter, not with an incomplete variable
  \item Like an $R^2$ for the missing data
  \item Most important diagnostic value for missing data problems
  \item Can only be computed after treating the missing data
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Covariance Coverage Examples}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      \begin{itemize}
      \item What is the coverage for $cov(X, Y)$?
        \vc
      \item What is the coverage for $cov(W, Y)$?
        \vc
      \item What about $cov(X, Z)$?
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, results = 'asis'>>=
tmpTab4 <- matrix(c(rep("w", 10), 
                    rep("x", 5), rep(".", 5),
                    rep("y", 10),
                    rep(".", 5), rep("z", 5)), 
                 ncol = 4)
colnames(tmpTab4) <- c("W", "X", "Y", "Z")

patTab4 <- xtable(tmpTab4, align = rep("c", 5))
print(patTab4, booktabs = TRUE)
@

\end{column}
\end{columns}
 
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Nonresponse Rate Examples}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      \begin{itemize}
        \item What is the percent missing at Time 2?
          \vc
        \item What is the attrition rate at Time 3?
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, results = 'asis'>>=
tmpTab5 <- matrix(c(rep("x1", 10), 
                    rep("x2", 7), rep(".", 3),
                    rep("x3", 5), rep(".", 5),
                    rep("x4", 3), rep(".", 7)), 
                 ncol = 4)
colnames(tmpTab5) <- c("T1", "T2", "T3", "T4")

patTab5 <- xtable(tmpTab5, align = rep("c", 5))
print(patTab5, booktabs = TRUE)
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}
  
  \begin{center}
    \huge{Missing Data Mechanisms}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Missing Data Mechanisms}

  Missing Completely at Random (MCAR):
  \begin{align*}
    P(R | Y_{mis}, Y_{obs}) = P(R)
  \end{align*}
  Missing at Random (MAR):
  \begin{align*}
    P(R | Y_{mis}, Y_{obs}) = P(R | Y_{obs})
  \end{align*}
  Missing Not at Random (MNAR):
  \begin{align*}
    P(R | Y_{mis}, Y_{obs}) \neq P(R | Y_{obs})
  \end{align*}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Simulate Some Toy Data}
  
<<>>=
nObs <- 5000 # Sample Size
pm   <- 0.3  # Proportion Missing

sigma <- matrix(c(1.0, 0.5, 0.0,
                  0.5, 1.0, 0.3,
                  0.0, 0.3, 1.0),
                ncol = 3)
simDat <- as.data.frame(rmvnorm(nObs, c(0, 0, 0), sigma))
colnames(simDat) <- c("y", "x", "z")

x <- simDat$x
y <- simDat$y
z <- simDat$z

cor(y, x) # Check correlation between X and Y
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{MCAR Example}
  
<<>>=
## Simulate MCAR Missingness:
rVec1 <- sample(1 : length(y), size = pm * length(y))

y2 <- y
y2[rVec1] <- NA

cor(y2, x, use = "pairwise") # Look at correlation
@ 

\pagebreak

<<echo = FALSE, out.width = "65%">>=
yDen  <- density(y)
y2Den <- density(y2, na.rm = TRUE)

pDat <- data.frame(y = c(yDen$y, y2Den$y),
                   x = c(yDen$x, y2Den$x),
                   g = rep(c("Complete", "MCAR w/ Deletion"),
                           each = length(yDen$y)
                           )
                   )


ggplot(data = pDat, mapping = aes(x = x, y = y, color = g)) +
    geom_line(size = 1) +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16)) +
    ylab("Density") +
    xlab("Value of Y") +
    scale_color_manual(values = c("blue", "red")) +
    theme(legend.position = c(0.8, 0.95), legend.title = element_blank())
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{MAR Example}
  
<<>>=
## Simulate MAR Missingness:
rVec2 <- x < quantile(x, probs = pm)
mean(rVec2)

y3 <- y
y3[rVec2] <- NA

cor(y3, x, use = "pairwise") # Not looking so good :(
@ 

\pagebreak

<<echo = FALSE, out.width = "65%">>=
y3Den <- density(y3, na.rm = TRUE)

pDat <- data.frame(y = c(yDen$y, y3Den$y),
                   x = c(yDen$x, y3Den$x),
                   g = rep(c("Complete", "MAR w/ Deletion"),
                           each = length(yDen$y)
                           )
                   )

marP <- ggplot(data = pDat, mapping = aes(x = x, y = y, color = g)) +
    geom_line(size = 1) +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16)) +
    ylab("Density") +
    xlab("Value of Y") +
    scale_color_manual(values = c("blue", "red")) +
    theme(legend.position = c(0.8, 0.95), legend.title = element_blank())
marP
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{MNAR Example}
  
<<>>=
## Simulate MNAR Missingness:
rVec3 <- y < quantile(y, probs = pm)
mean(rVec3)

y4 <- y
y4[rVec3] <- NA

cor(y4, x, use = "pairwise") # Hmm...looks pretty bad.
@ 

\pagebreak

<<echo = FALSE, out.width = "65%">>=
y4Den <- density(y4, na.rm = TRUE)

pDat <- data.frame(y = c(yDen$y, y4Den$y),
                   x = c(yDen$x, y4Den$x),
                   g = rep(c("Complete", "MNAR w/ Deletion"),
                           each = length(yDen$y)
                           )
                   )


ggplot(data = pDat, mapping = aes(x = x, y = y, color = g)) +
    geom_line(size = 1) +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16)) +
    ylab("Density") +
    xlab("Value of Y") +
    scale_color_manual(values = c("blue", "red")) +
    theme(legend.position = c(0.8, 0.95), legend.title = element_blank())
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Crucial Nuance}
  
  In our previous MAR example, ignoring the predictor of missingness actually 
  produces \emph{Indirect MNAR}.\\
  
  \pause
  \va
  
  \textsc{Question:} What happens if we ignore the predictor of missingness, but
  that predictor is independent of our study variables?
  
  \pause
  
<<>>=
rVec3     <- z < quantile(z, probs = pm)
y5        <- y
y5[rVec3] <- NA

cor(y5, x, use = "pairwise")
@ 

\textsc{Answer:} We get back to MCAR :)

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Crucial Nuance}
  
  The missing data mechanisms are not simply characteristics of an incomplete 
  dataset; we also need to account for the analysis.
  \vb
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, out.width = "100%", message = FALSE>>=
marP + scale_color_manual(values = c("blue", "red"), 
                          labels = c("Complete", "Indirect MNAR")
                          )
@ 

\end{column}
\begin{column}{0.5\textwidth}
  
<<echo = FALSE, out.width = "100%">>=
y5Den <- density(y5, na.rm = TRUE)

pDat <- data.frame(y = c(yDen$y, y5Den$y),
                   x = c(yDen$x, y5Den$x),
                   g = rep(c("Complete", "MCAR2"),
                           each = length(yDen$y)
                           )
                   )

ggplot(data = pDat, mapping = aes(x = x, y = y, color = g)) +
    geom_line(size = 1) +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16)) +
    ylab("Density") +
    xlab("Value of Y") +
    scale_color_manual(values = c("blue", "red")) +
    theme(legend.position = c(0.8, 0.95), legend.title = element_blank())
@
  
\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}
  
  \begin{center}
    \huge{Missing Data Treatments}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Bad Methods (These almost never work)}
   
  Listwise Deletion (Complete Case Analysis)
  \begin{itemize}
  \item Use only complete observations for the analysis
    \begin{itemize}
    \item Very wasteful (can throw out lots of useful data)
    \item Loss of statistical power
    \end{itemize}
  \end{itemize}

  \va
  
  Pairwise Deletion (Available Case Analysis)
  \begin{itemize}
  \item Use only complete pairs of observations for analysis
    \begin{itemize}
    \item Different samples sizes for different parameter estimates
    \item Can cause computational issues 
    \end{itemize}
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Bad Methods (These almost never work)}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      (Unconditional) Mean Substitution
      \begin{itemize}
      \item Replace $Y_{mis}$ with $\bigbar{Y}_{obs}$
        \begin{itemize}
        \item Negatively biases regression slopes and correlations
        \item Attenuates measures of linear association
        \end{itemize}
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, cache = TRUE, warning = FALSE>>=
dat1 <- readRDS(paste0(dataDir, "diabetes.rds"))
dat2 <- imposeMissData(data    = dat1,
                       targets = list(mar = c("ldl", "glu"),
                                      mcar = NA,
                                      mnar = NA),
                       preds   = "bmi",
                       pm      = list(mar = 0.25),
                       snr     = list(mar = 3),
                       pattern = "high")$data

dat3 <- dat2[ , c("bmi", "glu")]
rVec <- is.na(dat2$glu)

p0 <- ggplot(data = dat1, mapping = aes(x = bmi, y = glu)) +
    geom_point() +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16)) +
    xlab("BMI") +
    ylab("Blood Glucose")

p0
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Bad Methods (These almost never work)}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      (Unconditional) Mean Substitution
      \begin{itemize}
      \item Replace $Y_{mis}$ with $\bigbar{Y}_{obs}$
        \begin{itemize}
        \item Negatively biases regression slopes and correlations
        \item Attenuates measures of linear association
        \end{itemize}
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, cache = TRUE, warning = FALSE>>=
p1 <- p0 +
    geom_point(mapping = aes(y = dat2$glu, x = dat2$bmi), colour = "blue")

p1
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Bad Methods (These almost never work)}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      (Unconditional) Mean Substitution
      \begin{itemize}
      \item Replace $Y_{mis}$ with $\bigbar{Y}_{obs}$
        \begin{itemize}
        \item Negatively biases regression slopes and correlations
        \item Attenuates measures of linear association
        \end{itemize}
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, cache = TRUE, warning = FALSE>>=
miceM <- mice(data      = dat3,
              m         = 1,
              maxit     = 1,
              method    = "mean",
              printFlag = FALSE)

datM          <- complete(miceM, 1)
datM[!rVec, ] <- NA

p1 + geom_point(mapping = aes(y = datM$glu, x = datM$bmi), colour = "red")
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Bad Methods (These almost never work)} 

  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      Deterministic Regression Imputation\\
      (Conditional Mean Substitution)
      \begin{itemize}
      \item Replace $Y_{mis}$ with $\widehat{Y}_{mis}$ from some regression 
        equation
        \begin{itemize}
        \item Positively biases regression slopes and correlations
        \item Inflates measures of linear association
        \end{itemize}
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, cache = TRUE, warning = FALSE>>=
miceD <- mice(data      = dat3,
              m         = 1,
              maxit     = 10,
              method    = "norm.predict",
              printFlag = FALSE)

datD          <- complete(miceD, 1)
datD[!rVec, ] <- NA

p1 + geom_point(mapping = aes(y = datD$glu, x = datD$bmi), colour = "red")
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Bad Methods (These almost never work)} 

  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      Deterministic Regression Imputation\\
      (Conditional Mean Substitution)
      \begin{itemize}
      \item Replace $Y_{mis}$ with $\widehat{Y}_{mis}$ from some regression 
        equation
        \begin{itemize}
        \item Positively biases regression slopes and correlations
        \item Inflates measures of linear association
        \end{itemize}
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, cache = TRUE, warning = FALSE>>=
miceD <- mice(data      = dat2,
              m         = 1,
              maxit     = 10,
              method    = "norm.predict",
              printFlag = FALSE)

datD          <- complete(miceD, 1)
datD[!rVec, ] <- NA

p1 + geom_point(mapping = aes(y = datD$glu, x = datD$bmi), colour = "red")
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Bad Methods (These almost never work)}
  
  \begin{center}
    \ovalbox{General Issues with Deletion-Based Methods}
  \end{center}
  
  \begin{itemize}
  \item Biased parameter estimates unless data are MCAR
  \item Generalizability issues
  \end{itemize}
  
  \va
  
  \begin{center}
    \ovalbox{General Issues with Simple Single Imputation Methods}
  \end{center}

  \begin{itemize}
  \item Biased parameter estimates even when data are MCAR
  \item Attenuates variability in any treated variables
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Bad Methods (These almost never work)}
  
  Averaging Available Items (Person-Mean Imputation)
  \begin{itemize}
  \item Compute aggregate scores using only available values
    \begin{itemize}
    \item Missing data must be MCAR
    \item Each item must contribute equally to the aggregate score
    \end{itemize}
  \end{itemize}
  
  \vb
  
  Last Observation Carried Forward (LOCF)
  \begin{itemize}
  \item Replace post-dropout values with the most recent observed value
    \begin{itemize}
    \item Assume that dropouts would maintain their last known values
    \item Attenuates estimates of growth/development
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{OK Methods (These work in some situations)}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      Stochastic Regression Imputation
      \vc
      \begin{itemize}
      \item Fill $Y_{mis}$ with $\widehat{Y}_{mis}$ plus some random noise.
        \vc
        \begin{itemize}
        \item Produces unbiased parameter estimates and predictions
          \vc
        \item Computationally efficient
          \vc
        \item Attenuates standard errors
          \vc
        \item Makes CIs and prediction intervals too narrow
        \end{itemize}
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, cache = TRUE, warning = FALSE>>=
miceS <- mice(data      = dat3,
              m         = 1,
              maxit     = 10,
              method    = "norm.nob",
              printFlag = FALSE)

datS          <- complete(miceS, 1)
datS[!rVec, ] <- NA

p1 + geom_point(mapping = aes(y = datS$glu, x = datS$bmi), colour = "red")
@ 

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{OK Methods (These work in some situations)}
  
  Nonresponse Weighting
  \vc
  \begin{itemize}
  \item Weight the observed cases to correct for nonresponse bias
    \vc
    \begin{itemize}
    \item Popular in survey research and official statistics
      \vc
    \item Only worth considering with \emph{Unit Nonresponse}
      \vc
    \item Doesn't make any sense with \emph{Item Nonresponse}
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Good Methods (These almost always work)}
  
  Multiple Imputation (MI)
  \vc
  \begin{itemize}
  \item Replace the missing values with $M$ plausible estimates.
    \vc
    \begin{itemize}
    \item Essentially, a repeated application of stochastic regression 
      imputation (with a particular type of regression model)
      \vc
    \item Produces unbiased parameter estimates and predictions.
      \vc
    \item Produces ``correct'' standard errors, CIs, and prediction intervals.
      \vc
    \item Very, very flexible
      \vc
    \item Computationally expensive
    \end{itemize}
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[allowframebreaks, fragile]{Good Methods (These almost always 
    work)}

  What happens when we apply MI to our previous MAR example?
<<>>=
## Estimate imputation model:
miceOut1 <- mice(data      = data.frame(y3, x),
                 m         = 100,
                 maxit     = 1,
                 method    = c("norm", ""),
                 printFlag = FALSE)

## Replace missing values with imputations:
impList1 <- list()
for(m in 1 : miceOut1$m) 
    impList1[[m]] <- complete(miceOut1, m)
@ 

\pagebreak

<<>>=
## Estimate M correlations:
corList <-lapply(impList1,
                 FUN = function(impDat)
                     cor(impDat$x, impDat$y3)
                 )

## Pool estimates:
mean(unlist(corList))
@ 

The MI-based parameter estimate looks good.
\begin{itemize}
\item MI produces unbiased estimates of the parameter when data are MAR.
\end{itemize}

\pagebreak

<<echo = FALSE, out.width = "65%">>=
impX <- impY <- list()
for(m in 1 : 100) {
    tmp       <- density(impList1[[m]]$y3)
    impX[[m]] <- tmp$x
    impY[[m]] <- tmp$y
}

pDat <- data.frame(x = unlist(impX),
                   y = unlist(impY),
                   g = rep(1 : 100, each = length(impX[[1]])),
                   c = "MAR w/ MI")

pDat <- rbind.data.frame(pDat,
                         data.frame(x = yDen$x, 
                                    y = yDen$y, 
                                    g = 101, 
                                    c = "Complete")
                         )

ggplot(data    = pDat, 
       mapping = aes(x = x, y = y, color = c, group = g, size = c)
       ) +
    geom_line() +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16)) +
    ylab("Density") +
    xlab("Value of Y") +
    scale_color_manual(values = c("red", "black")) +
    scale_size_manual(values = c(0.5, 1)) +
    theme(legend.position = c(0.8, 0.95), legend.title = element_blank())
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Good Methods (These almost always work)}
  
  What about applying MI to our MNAR example?
<<>>=
## Estimate imputation model:
miceOut2 <- mice(data      = data.frame(y4, x),
                 m         = 100,
                 maxit     = 1,
                 method    = c("norm", ""),
                 printFlag = FALSE)

## Replace missing values with imputations:
impList2 <- list()
for(m in 1 : miceOut2$m) 
    impList2[[m]] <- complete(miceOut2, m)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks, fragile]{Good Methods (These \emph{almost} 
    always work)}

<<>>=
## Estimate M correlations:
corList2 <-lapply(impList2,
                 FUN = function(impDat)
                     cor(impDat$x, impDat$y4)
                 )

## Pool estimates:
mean(unlist(corList2))
@ 

The MI-based parameter estimate is still biased.
\begin{itemize}
\item MI cannot correct bias in parameter estimates when data are MNAR.
\end{itemize}

\pagebreak

<<echo = FALSE, out.width = "65%">>=
y4Den <- density(y4, na.rm = TRUE)

impX <- impY <- list()
for(m in 1 : 100) {
    tmp       <- density(impList2[[m]]$y4)
    impX[[m]] <- tmp$x
    impY[[m]] <- tmp$y
}

pDat <- data.frame(x = unlist(impX),
                   y = unlist(impY),
                   g = rep(1 : 100, each = length(impX[[1]])),
                   c = "MNAR w/ MI")

pDat <- rbind.data.frame(pDat,
                         data.frame(x = y4Den$x, y = y4Den$y, g = 101, c = "MNAR w/ Deletion"),
                         data.frame(x = yDen$x, y = yDen$y, g = 102, c = "Complete")
                         )

ggplot(data = pDat, mapping = aes(x = x, y = y, color = c, group = g, size = c)) +
    geom_line() +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16)) +
    ylab("Density") +
    xlab("Value of Y") +
    scale_color_manual(values = c("red", "blue", "black")) +
    scale_size_manual(values = c(0.5, 1, 1)) +
    theme(legend.position = c(0.8, 0.95), legend.title = element_blank())
@ 

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Good Methods (These almost always work)}

  Bayesian Modeling
  \vc
  \begin{itemize}
  \item Treat missing values as just another parameter to be estimated.
    \vc
    \begin{itemize}
    \item Models can be directly estimated in the presence of missing data.
      \begin{itemize}
      \item Essentially, runs MI behind-the-scenes during model estimation
      \end{itemize}
      \vc
    \item The predictors of nonresponse must be included in the model, somehow.
      \vc
    \item Computationally expensive
    \end{itemize}
  \end{itemize}
  
  \pagebreak
  
  Full Information Maximum Likelihood (FIML)
  \vc
  \begin{itemize} 
  \item Adjust the objective function to only consider the observed parts of the 
    data.
    \vc
    \begin{itemize}
    \item Models are directly estimated in the presence of missing data.
      \vc
    \item The predictors of nonresponse must be included in the model, somehow.
      \vc
    \item Unless you write your own optimization program, FIML is only available 
      for certain types of models.
      \vc
    \item In linear regression models, FIML cannot treat missing data on 
      predictors (if the predictors are taken as fixed).
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%
  
\begin{frame}{}
  
  \begin{center}
    \Huge{\textsc{Outliers}}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{What is an outlier?}
  
  For the time being, we're considering \emph{univariate outliers}.
  \vb
  \begin{itemize}
  \item Extreme values with respect to the distribution of a variable's other 
    observations
    \vc
    \begin{itemize}
    \item A human height measurement of 3 meters
      \vc
    \item A high temperature in Tilburg of $50^\circ$
      \vc
    \item Annual income of \euro250,000 for a student
    \end{itemize}
    \vb
  \item Not accounting for any particular model (we'll get to that later)
  \end{itemize}
  
  \pagebreak
  
  A univariate outlier may, or may not, be an illegal value.
  \vb
  \begin{itemize}
  \item Data entry errors are probably the most common cause.
    \vc
  \item Outliers can also be legal, but extreme, values.
  \end{itemize}
  
  \va
  
  \textsc{Key Point:} We choose to view an outlier as arising from a different 
  population than the one to which we want to generalize our findings.
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Finding Univariate Outliers}

  We have many methods available to diagnose potential outliers.
  \vb
  \begin{itemize}
  \item Four of the simplest and most popular are:
    \vc
    \begin{enumerate}
    \item Internally studentized residuals (AKA Z-score method)
      \vc
    \item Externally studentized residuals
      \vc
    \item Median absolute deviation method
      \vc
    \item Tukey's boxplot method
    \end{enumerate}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Internally Studentized Residuals}

  For each observation, $X_n$, we compute the following quantity:
  \begin{align*}
    T_n = \frac{X_n - \bigbar{X}}{SD_X}
  \end{align*}
  \vx{-12}
  \begin{itemize}
  \item $T_n$ follows a Student's $t$ distribution with $df = N - 1$.
    \begin{itemize}
    \item We can do a formal test for ``outlier'' status.
    \end{itemize}
  \item Assuming a large sample, if $T_n > C$ (where $C$ is usually 2 or 3), we 
    label $X_n$ as an outlier.
  \end{itemize}
  
  \vb
  \pause
  
  Although simple, this method has some substantial limitations.
  \begin{itemize}
  \item The cutpoint, $C$, can only be meaningfully chosen when $X$ is normally 
    distributed.
  \item Both $\bigbar{X}$ and $SD_X$ are highly sensitive to outliers.
  \end{itemize}
  
\end{frame}


%------------------------------------------------------------------------------%

\begin{frame}{Externally Studentized Residual}

  The externally studentized residual method is essentially the same as the 
  internally studentized residual method, but we adjust $\bigbar{X}$ and $SD_X$ 
  to remove the influence of the observation we're evaluating.
  \vb
  \begin{itemize}
  \item Let $\mathbb{N}_{(n)} = \{1, \ldots, (n - 1), (n + 1), \ldots, N\}$. 
  \item Define the deletion mean, $\bigbar{X}_{(n)}$, and deletion SD, 
    $SD_{X(n)}$, as:
    \begin{align*}
      \bigbar{X}_{(n)} &= \frac{1}{N - 1} \sum_{i \in \mathbb{N}_{(n)}} X_i\\
      SD_{X(n)} &= \sqrt{\frac{1}{N - 2} \sum_{i \in \mathbb{N}_{(n)}} \left(X_i - \bigbar{X}_{(n)}\right)^2}
    \end{align*}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Externally Studentized Residual}
  
  The externally studentized residual is defined in the same way as the 
  internally studentized version:
  \begin{align*}
    T_{(n)} = \frac{X_n - \bigbar{X}_{(n)}}{SD_{X(n)}}
  \end{align*}
  \vx{-12}
  \begin{itemize}
  \item $T_{(n)}$ follows a Student's $t$ distribution with $df = N - 2$.
    \begin{itemize}
    \item We can do a formal test for ``outlier'' status.
    \end{itemize}
  \item Assuming a large sample, if $T_{(n)} > C$ (where $C$ is usually 2 or 3), 
    we label $X_n$ as an outlier.
  \end{itemize}
  
  \vb
  \pause
  
  $T_{(n)}$ is immune to the influence of the $n$th observation.
  \begin{itemize}
  \item Still requires $X$ to be normally distributed
  \item Still sensitive to outliers other than the $n$th observation
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Median Absolute Deviation Method}
  
<<echo = FALSE>>=
q <- qnorm(0.75)
b <- 1 / q
@ 

The biggest limitation of studentized residuals is that their measures of 
central tendency and dispersion are sensitive to outliers.
\vb
\begin{itemize}
\item If we can replace the (deletion) mean and the (deletion) SD with more 
  robust statistics, we can avoid this issue.
  \vb
  \begin{itemize}
  \item Replace the mean, $\bar{X}$, with the \emph{median}, $\text{Med}(X)$
    \vb
  \item Replace the SD with the \emph{median absolute deviation}:
    \begin{align*}
      MAD_X = b \times \text{Med} \left( \big| X_n - \text{Med} (X) \big| 
      \right)
    \end{align*}
  \item We choose the coefficient as $b = 1/Q_{0.75}$
    \vb
  \item For the normal distribution, $b \approx 1/\Sexpr{round(q, 4)} \approx 
    \Sexpr{round(b, 4)}$
  \end{itemize}
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Median Absolute Deviation Method}
  
  We compute our test statistic by replacing the mean with the median and the SD 
  with the MAD in the standard Wald test formula:
  \begin{align*}
    T_{MAD} = \frac{X_n - \text{Med}(X)}{MAD_X}
  \end{align*}
  \vx{-12}
  \begin{itemize}
  \item $T_{MAD}$ doesn't allow for formal statistical tests.
  \item We can use the same general cutoffs we would use for the studentized 
    residual methods.
    \begin{itemize}
    \item Assuming a large sample, if $T_{(n)} > C$ (where $C$ is usually 2 or 
      3), we label $X_n$ as an outlier.
    \end{itemize}
  \end{itemize}
  
  \vb
  \pause
  
  $T_{MAD}$ is immune to the influence of, up to, 50\% outlying observations.
  \begin{itemize}
  \item Requires us to assume a parametric distribution for $X$
    \begin{itemize}
    \item This assumption is necessary to compute $b$.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Breakdown Point}
  
  To compare robust statistics, we consider their \emph{breakdown points}.
  \begin{itemize}
  \item The breakdown point is the minimum proportion of cases that must be 
    replaced by $\infty$ to cause the value of the statistic to go to $\infty$.
  \end{itemize}
  \vc
  The mean has a breakdown point of $1 / N$.
  \begin{itemize}
  \item Replacing a single value with $\infty$ will produce an infinite mean.
  \end{itemize}
  \vc
  The deletion mean has a breakdown point of $2 / N$.
  \begin{itemize}
  \item We can replace, at most, 1 value with $\infty$ without producing an 
    infinite mean.
  \end{itemize}
  \vc
  The median has breakdown point of 50\%.
  \begin{itemize}
    \item We can replace $n < N / 2$ of the observations with $\infty$ without 
      producing an infinite median.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Boxplot Method}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      \citet{tukey:1977} described a procedure for flagging potential outliers 
      based on the familiar box-and-whiskers plot.
      \begin{itemize}
      \item Does not require normally distributed $X$
      \item Not sensitive to outliers
      \item Doesn't allow for formal statistical tests
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
data(Salaries)

boxplot(salary ~ sex,
        data = Salaries,
        ylab = "Salary",
        main = "9-Month Salaries of Professors")
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Boxplot Method}
  
  A \emph{fence} is an interval defined as the following function of the 
  \emph{first quartile}, the \emph{third quartile}, and the \emph{inner quartile
    range} ($IQR = Q_3 - Q_1$):
  \begin{align*}
    F = \{Q_1 - C \times IQR, Q_3 + C \times IQR\}
  \end{align*}
  
  \vx{-6}
  
  \begin{itemize}
  \item Taking $C = 1.5$ produces the \emph{inner fence}.
  \item Taking $C = 3.0$ produces the \emph{outer fence}.
  \end{itemize}
  
  \vb
  
  We can use these fences to identify potential outliers:
  \begin{itemize}
  \item Any value that falls outside of the inner fence is a \emph{possible 
    outlier}.
  \item Any value that falls outside of the outer fence is a \emph{probable 
    outlier}.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Multivariate Outliers}
  
  Sometimes, the combinations of values in an observation are very unlikely, 
  even when no individual value is an outlier.
  \vc
  \begin{itemize}
  \item These observations are \emph{multivariate outliers}.
    \vc
    \begin{itemize}
    \item A person in the 95\emph{th} percentile for height and the 5\emph{th} 
      percentile for weight
      \vc
    \item A person who simultaneously scores highly on scales of depression and 
      positive affect
    \end{itemize}
  \end{itemize}
  \vb
  To detect multivariate outliers, we use \emph{distance metrics}.
  \vc
  \begin{itemize}
  \item Distance metrics quantify the similarity of two vectors.
    \vc
    \begin{itemize}
    \item Similarity between two observations
      \vc
    \item Similarity between an observation and the mean vector
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Mahalanobis Distance}
  
  One of the most common distance metrics is the \emph{Mahalanobis Distance}.
  \vc
  \begin{itemize}
  \item The Mahalanobis distance, $\Delta$, is a multivariate generalization of 
    the internally studentized residual:
  \end{itemize}
  \begin{align*}
    \Delta_n = \sqrt{\left( \mathbf{x}_n - \hat{\mu}_{\mathbf{X}} \right) 
      \hat{\Sigma}_{\mathbf{X}}^{-1} \left( \mathbf{x}_n - \hat{\mu}_{\mathbf{X}} 
      \right)^T}
  \end{align*}
  As with studentized residuals, if $\Delta_n > C$, we label $\mathbf{x}_n$ as 
  an outlier.
  \vc
  \begin{itemize}
  \item When $\mathbf{X}$ is $K$-variate normally distributed, $\Delta_n^2$ 
    follows a $\chi^2$ distribution with $df = K$.
    \vc
  \item We take $C$ to be the square-root of a suitably conservative quantile 
    (e.g., $q \in \{99\%, 99.9\%\}$) of the $\chi_K^2$ distribution: 
    $C = \sqrt{\chi_{K, q}^2}$.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Problems with Mahalanobis Distance}
  
  Like the internally studentized residual, Mahalanobis distance is highly 
  sensitive to outliers.
  \vc
  \begin{itemize}
  \item The underlying estimates of central tendency, $\hat{\mu}_{\mathbf{X}}$, 
    and dispersion, $\hat{\Sigma}_{\mathbf{X}}$, are computed using all 
    observations.
  \end{itemize}
  \vb
  \pause
  We want robust analogues of $\hat{\mu}_{\mathbf{X}}$ and 
  $\hat{\Sigma}_{\mathbf{X}}$.
  \vc
  \begin{itemize}
  \item We have several options for robust estimation of $\hat{\mu}_{\mathbf{X}}$ 
    and $\hat{\Sigma}_{\mathbf{X}}$. E.g.:
    \begin{itemize}
    \item Minimum covariance determinant method \citep[MCD; ][]{rousseeuw:1985}
      \vc
    \item Minimum volume ellipsoid method \citep[MVE; ][]{rousseeuw:1985}
      \vc
    \item M-estimation \citep{maronna:1976}
    \end{itemize}
    \vc
  \item Conceptually, robust methods operate by either:
    \begin{itemize}
    \item Using only a ``good'' subset of data to estimate $\hat{\mu}_{\mathbf{X}}$ 
      and $\hat{\Sigma}_{\mathbf{X}}$.
      \vc
    \item Downweighting outlying observations when estimating 
      $\hat{\mu}_{\mathbf{X}}$ and $\hat{\Sigma}_{\mathbf{X}}$.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Robust Mahalanobis Distance}
  
  Equipped with robust estimates of central tendency, $\hat{\mu}_{R,\mathbf{X}}$, 
  and dispersion, $\hat{\Sigma}_{R,\mathbf{X}}$, we define the robust Mahalanobis 
  distance in the natural way:
  \begin{align*}
    \Delta_{R,n} = \sqrt{\left( \mathbf{x}_n - \hat{\mu}_{R,\mathbf{X}} \right) 
      \hat{\Sigma}_{R,\mathbf{X}}^{-1} \left( \mathbf{x}_n - \hat{\mu}_{R,\mathbf{X}} 
      \right)^T}
  \end{align*}
  We use $\Delta_{R,n}$ in the same way as $\Delta_n$.
  \vc
  \begin{itemize}
  \item If $\Delta_{R,n} > C$, we label $\mathbf{x}_n$ as an outlier.
    \vc
  \item Again, we take $C$ to be the square-root of some quantile of the 
    $\chi_K^2$ distribution: $C = \sqrt{\chi_{K, q}^2}$.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Practicalities: Univariate vs. Multivariate}

  Univariate outlier checks are safe for most variables.\\ 
  \vb
  \pause
  \textsc{\underline{Don't}} include too many variables in multivariate outlier 
  checks.
  \begin{itemize}
  \item More variables increases the chances of false positives.
  \item E.g., don't run a multivariate outlier test on your entire dataset.
  \end{itemize}
  \vb
  \pause
  \textsc{\underline{Do}} use multivariate outlier checks for scales.
  \begin{itemize}
  \item E.g., if you have a psychometric scale measuring depression, you should
    check the items of that scale for multivariate outliers.
  \end{itemize}
  \vb 
  \pause
  \textsc{\underline{Maybe}} check the variables in a single model for
  multivariate outliers.
  \begin{itemize}
  \item E.g., if you have a small set of items that you will include in a
    regression model, it could make sense to check these variables for
    multivariate outliers.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Practicalities: Outliers for Categorical Data}

  Nominal, ordinal, and binary items \emph{can} have outliers.
  \begin{itemize}
  \item Outliers on categorical variables are often more indicative of bad
    variables than outlying cases.
  \end{itemize}
  \vb
  \pause
  Ordinal
  \begin{itemize}
  \item Most participant endorse one of the lowest categories on an ordinal
    item, but a few participants endorse the highest category.
  \item The participants who endorse the highest category may be outliers.
  \end{itemize}
  \vb
  \pause
  Nominal
  \begin{itemize}
  \item Groups with very low membership may be outliers on nominal grouping
    variables.
  \end{itemize}
  \vb
  \pause
  Binary
  \begin{itemize}
  \item If most endorse the item, the few who do not may be outliers.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Treating Outliers}
  
  If we locate any outliers, they must be treated.
  \vc
  \begin{itemize}
  \item Outliers cause by errors, mistakes, or malfunctions (i.e., \emph{error 
    outliers}) should be directly corrected.
    \vc
  \item Labeling non-error outliers is a subjective task.
    \begin{itemize}
    \item A (non-error) outlier must originate from a population separate from 
      the one we care about.
    \item Don't blindly automate the decision process.
    \end{itemize}
  \end{itemize}
  
  \pause
  \vb
 
  The most direct solution is to delete any outlying observation.
  \vc
  \begin{itemize}
  \item If you delete non-error outliers, the analysis should be reported twice: 
    with outliers and without.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Treating Outliers}
  
  For univariate outliers, we can use less extreme types of deletion.
  \begin{itemize}
  \item Delete outlying values (but not the entire observation).
  \item These empty cells then become missing data.
  \end{itemize}
  \vb
  Winsorization:
  \begin{itemize}
  \item Replace the missing values with the nearest non-outlying value.
  \end{itemize}
  \vb
  Missing data analysis:
  \begin{itemize}
  \item Treat the missing values along with any naturally-occurring nonresponse.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Treating Outliers}
  
  We can also use robust regression procedures to estimate the model directly in 
  the presence of outliers.
  \vc
  \begin{itemize}
  \item Weight the objective function to reduce the impact of outliers
    \begin{itemize}
    \item M-estimation
    \end{itemize}
    \vc
  \item Trim outlying observations during estimation 
    \begin{itemize}
    \item Least trimmed squares, MCD, MVE
    \end{itemize}
    \vc
  \item Take the median, instead of the mean, of the squared residuals
    \begin{itemize}
    \item Least median of squares
    \end{itemize}
    \vc
  \item Model some quantile of the DV's distribution instead of the mean 
    \begin{itemize}
    \item Quantile regression
    \end{itemize}
    \vc
  \item Model the outcome with a heavy-tailed distribution
    \begin{itemize}
    \item Laplacian, Student's T
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Conclusion}
  
  When cleaning our data, two of the biggest issues we'll need to address are 
  missing data and outliers.
  \vc
  \begin{itemize}
  \item Missing data are unobserved values that should be present.
    \vc
  \item Outliers are observations that originate from a different population 
    than our target.
    \begin{itemize}
    \item Outliers can be univariate or multivariate.
    \end{itemize}
  \end{itemize}
  \vb
  When we find missing data, we must treat them carefully.
  \vc
  \begin{itemize}
  \item Common, simple methods tend to lead to problems.
    \vc
  \item Good methods carefully consider the distribution of missing values.
  \end{itemize}
  
  \pagebreak
  
  When checking for outliers, we want to use robust methods.
  \vc
  \begin{itemize}
  \item Some outlier checks are sensitive to outliers, themselves.
    \vc
  \item We want to use methods that estimate central tendency and dispersion from 
    only ``good'' data.
  \end{itemize}
  \vb
  Labeling outliers that are not produced by obvious errors is a subjective 
  process.
  \vc
  \begin{itemize}
  \item The outlying status of a non-error outlier must be justified.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{References}

  \bibliographystyle{apacite}
  \bibliography{../../../literature/bibtexFiles/statMethRefs.bib}

\end{frame}

%------------------------------------------------------------------------------%

\end{document}
