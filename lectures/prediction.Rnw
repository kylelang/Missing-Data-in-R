%%% Title:    DSS Stats & Methods: Lecture 6
%%% Author:   Kyle M. Lang
%%% Created:  2017-09-12
%%% Modified: 2020-09-18

\documentclass{beamer}
\usetheme[%
  pageofpages          = of,
  bullet               = circle,
  titleline            = true,
  alternativetitlepage = true,
  titlepagelogo        = Logo3,
  watermark            = watermarkTiU,
  watermarkheight      = 100px,
  watermarkheightmult  = 4%
]{UVT}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{fancybox}
\usepackage{caption}

\newcommand{\kfold}[0]{\emph{K}-fold cross-validation}

%% Ensure styles of `blocks' (used in Definitions, Theorems etc.) follows the
%% UVT-style theme:
\setbeamercolor{block title}{fg = darkblue, bg = white}
\setbeamercolor{block body}{use = block title, bg = block title.bg}

%% Ensure TableOfContents is in UVT-style theme:
\setbeamercolor{section in toc}{fg = darkblue}

%% Don't label tables:
\captionsetup[table]{labelformat = empty}

\title{Prediction, Imputation, \& Cross-Validation}
\subtitle{Statistics \& Methodology Lecture 6}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Tilburg University}
\date{}

\begin{document}

%------------------------------------------------------------------------------%

<<setup, include=FALSE>>=
set.seed(235711)

library(knitr)
library(ggplot2)
library(MASS)
library(mvtnorm)
library(glmnet)
library(xtable)
library(pscl)
library(LaplacesDemon)
library(MLmetrics)

source("../../../code/supportFunctions.R")
dataDir <- "../data/"

opts_chunk$set(size = 'footnotesize', fig.align = 'center')
knit_theme$set('edit-kwrite')

lightBlue <- rgb(0,   137, 191, max = 255)
midBlue   <- rgb(0,   131, 183, max = 255)
darkBlue  <- rgb(0,   128, 179, max = 255)
deepGold  <- rgb(184, 138, 45,  max = 255)
lightGold <- rgb(195, 146, 48,  max = 255)
@

%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
\titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  
  \begin{enumerate}
  \item Prediction
    \va
  \item Cross-validation
    \va
  \item Missing data imputation
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Prediction}
  
  So far, we've focused primarily on inferences about the estimated regression
  coefficients.  
  \vb
  \begin{itemize}
  \item Asking questions about how $\mathbf{X}$ is related to $Y$
  \end{itemize}
  \vb
  We can also use linear regression for \emph{prediction}.
  \vb
  \begin{itemize}
  \item Given a new observation, $X_m$, what outcome value, $\hat{Y}_m$, does
    our model attribute to the $m$th observation?
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Prediction}
  
  Train a model to predict employee performance using features extracted from 
  CVs.
  \begin{itemize}
  \item When screening applicants for a new position, use the data in their CVs 
    to predict their expected performance.  
  \end{itemize}
  \vb
  Predict recidivism risk based on personal history, criminal history, and 
  in-prison behavior record.
  \begin{itemize}
  \item When evaluating a parole application, calculate the predicted chance of 
    recidivism.
  \end{itemize}
  \vb
  Predict future gasoline prices based on geo-political events in
  oil-producing countries.  
  \begin{itemize}
    \item If conflict escalates in the Middle East, adjust the appropriate
      features and project likely changes in gasoline prices.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Prediction Example}
  
<<echo = FALSE>>=
diabetes <- readRDS("../data/diabetes.rds")

trainDat <- diabetes[1 : 400, ]
testDat  <- diabetes[401 : 442, ]

out1 <- lm(ldl ~ bp + glu + bmi, data = trainDat)
b0 <- round(coef(out1)[1], 3)
b1 <- round(coef(out1)[2], 3)
b2 <- round(coef(out1)[3], 3)
b3 <- round(coef(out1)[4], 3)

x1 <- testDat[1, "bp"]
x2 <- testDat[1, "glu"]
x3 <- testDat[1, "bmi"]

confInt <- round(
    predict(out1, newdat = testDat, interval = "confidence")[1, 2 : 3], 3
)
predInt <- round(
    predict(out1, newdat = testDat, interval = "prediction")[1, 2 : 3], 3
)
@ 

To fix ideas, let's reconsider the \emph{diabetes} data and the following model:
\begin{align*}
  Y_{LDL} = \beta_0 + \beta_1 X_{BP} + \beta_2 X_{gluc} + \beta_3 X_{BMI} + 
  \varepsilon
\end{align*}
Training this model on the first $N = 400$ patients' data produces the following
fitted model:
\begin{align*}
  \hat{Y}_{LDL} = \Sexpr{b0} + \Sexpr{b1} X_{BP} + \Sexpr{b2} X_{gluc} + 
  \Sexpr{b3} X_{BMI}
\end{align*}
\pause
Suppose a new patient presents with $BP = \Sexpr{x1}$, $gluc = \Sexpr{x2}$, and
$BMI = \Sexpr{x3}$. We can predict their $LDL$ score by:
\begin{align*}
  \hat{Y}_{LDL} &= \Sexpr{b0} + \Sexpr{b1} (\Sexpr{x1}) + \Sexpr{b2} 
  (\Sexpr{x2}) + \Sexpr{b3} (\Sexpr{x3})\\
  &= \Sexpr{round(predict(out1, testDat[1 : 2, ])[1], 3)}
\end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Prediction with Centered X Variables}
  
<<echo = FALSE>>=
diabetes$bp90   <- diabetes$bp - 90
diabetes$glu100 <- diabetes$glu - 100
diabetes$bmi30  <- diabetes$bmi - 30

trainDat <- diabetes[1 : 400, ]
testDat  <- diabetes[401 : 442, ]

out1 <- lm(ldl ~ bp90 + glu100 + bmi30, data = trainDat)

b0 <- round(coef(out1)[1], 3)
b1 <- round(coef(out1)[2], 3)
b2 <- round(coef(out1)[3], 3)
b3 <- round(coef(out1)[4], 3)

x1 <- testDat[1, "bp"]
x2 <- testDat[1, "glu"]
x3 <- testDat[1, "bmi"]

x1.2 <- testDat[1, "bp90"]
x2.2 <- testDat[1, "glu100"]
x3.2 <- testDat[1, "bmi30"]

pred0 <- t(matrix(c(1, x1, x2, x3))) %*% matrix(coef(out1))
pred1 <- t(matrix(c(1, x1.2, x2.2, x3.2))) %*% matrix(coef(out1))
@ 

Suppose we fit the preceding model with $BP$ centered at 90, $gluc$ centered at 
100, and $BMI$ centered at 30.
\begin{itemize}
\item We'd get the following fitted model:
\end{itemize}
\begin{align*}
  \hat{Y}_{LDL} = \Sexpr{b0} + \Sexpr{b1} X_{BP.90} + \Sexpr{b2} X_{gluc.100} + \Sexpr{b3} X_{BMI.30}
\end{align*}
\pause
Now, let's generate predictions for our patient with $BP = \Sexpr{x1}$, 
$gluc = \Sexpr{x2}$, and $BMI = \Sexpr{x3}$:
\begin{align*}
  \hat{Y}_{LDL} &= \Sexpr{b0} + \Sexpr{b1} (\Sexpr{x1}) + \Sexpr{b2} 
  (\Sexpr{x2}) + \Sexpr{b3} (\Sexpr{x3})\\
  &= \Sexpr{round(pred0, 3)}
\end{align*}
\pause
To get the correct prediction, we need to plug-in the centered X values:
\begin{align*}
  \hat{Y}_{LDL} &= \Sexpr{b0} + \Sexpr{b1} (\Sexpr{x1} - 90) + \Sexpr{b2} 
  (\Sexpr{x2} - 100) + \Sexpr{b3} (\Sexpr{x3} - 30)\\
  &= \Sexpr{round(pred1, 3)}
\end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Interval Estimates for Prediction}
  
  To quantify uncertainty in our predictions, we want to use an appropriate
  interval estimate.  
  \vb
  \begin{itemize}
  \item Two flavors of interval are applicable to predictions:
    \begin{enumerate}
    \item Confidence intervals for $\hat{Y}$
      \vc
    \item Prediction intervals for a specific observation
    \end{enumerate}
    \vb
  \item CIs for $\hat{Y}$ give a likely range (in the sense of coverage 
    probability and ``confidence'') for the true conditional mean of $Y$, 
    $\mu_{Y|X^*}$.
    \begin{itemize}
    \item They only account for uncertainty in the estimated regression 
      coefficients, $\{\hat{\beta}_0, \hat{\beta}_p\}$.
    \end{itemize}
    \vb
  \item Prediction intervals give a likely range (in the same sense as CIs) for 
    future outcome values, $Y^*$.
    \begin{itemize}
    \item They also account for the regression errors, $\varepsilon$. 
    \end{itemize}
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Confidence vs. Prediction Intervals}
  
 \begin{columns}
   \begin{column}{0.5\textwidth}
     
     Let's visualize the predictions from a simple model:
     \begin{align*}
       Y_{BP} = {\color{blue} \hat{\beta}_0 + \hat{\beta}_1 X_{BMI}} + 
       {\color{red} \hat{\varepsilon}}
     \end{align*}
     \vx{-12}
     \begin{itemize}
     \item<2-3> CIs for $\hat{Y}$ ignore the errors, ${\color{red}\varepsilon}$.
       \begin{itemize}
       \item They only care about the best-fit line, 
         ${\color{blue} \beta_0 + \beta_1 X_{BMI}}$.
       \end{itemize}
       \vb
     \item<3> Prediction intervals are wider than CIs.
       \begin{itemize}
       \item They account for the additional uncertainty contributed by 
         ${\color{red}\varepsilon}$.
       \end{itemize}
     \end{itemize}
     
   \end{column}
   \begin{column}{0.5\textwidth}
     
<<echo = FALSE, cache = TRUE>>=
out1 <- lm(bp ~ bmi, data = trainDat)

testDat$preds <- predict(out1, newdata = testDat)

ci <- predict(out1, newdata = testDat, interval = "confidence")[ , -1]
pi <- predict(out1, newdata = testDat, interval = "prediction")[ , -1]

colnames(ci) <- c("ciLo", "ciHi")
colnames(pi) <- c("piLo", "piHi")

testDat <- data.frame(testDat, ci, pi)

p1 <- ggplot(data = testDat, aes(x = bmi, y = bp)) +
    theme_classic() +
    theme(text = element_text(size = 16, family = "Courier")) + 
    geom_segment(aes(x = bmi, y = bp, xend = bmi, yend = preds),
                 color = "red") +
    geom_line(mapping = aes(x = bmi, y = preds), color = "blue", size = 1) +
    geom_point() +
    ylim(range(pi))
@ 
\only<1>{
<<echo = FALSE, cache = TRUE>>=
print(p1)
@ 
}
\only<2>{
<<echo = FALSE, cache = TRUE>>=
p2 <- p1 +
    geom_line(mapping = aes(x = bmi, y = ciLo), size = 1, linetype = "solid") +
    geom_line(mapping = aes(x = bmi, y = ciHi), size = 1, linetype = "solid") 
p2
@ 
}
\only<3>{
<<echo = FALSE, cache = TRUE, warning = FALSE>>=
p2 + 
    geom_line(mapping = aes(x = bmi, y = piLo), size = 1, linetype = "dashed") +
    geom_line(mapping = aes(x = bmi, y = piHi), size = 1, linetype = "dashed")
@
}

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%
 
\begin{frame}{Confidence vs. Prediction Intervals}
  
  Assume we want predictions for a new observation, $X^*$. Then our intervals 
  can be computed as follows:
  \vb
  \begin{itemize}
  \item Confidence interval:
    \begin{align*}
      CI = \hat{Y} \pm t_{crit} \sqrt{\frac{\hat{\sigma}^2}{N} + \frac{\left(X^* - \bar{X}\right)^2}{\left(\sum_{n = 1}^N X_n- \bar{X}\right)^2}}
    \end{align*}
  \item Prediction interval:
    \begin{align*}
      PI = \hat{Y} \pm t_{crit} \sqrt{{\color{red}\hat{\sigma}^2} + \frac{\hat{\sigma}^2}{N} + \frac{\left(X^* - \bar{X}\right)^2}{\left(\sum_{n = 1}^N X_n- \bar{X}\right)^2}}
    \end{align*}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Interval Estimates Example}
  
  Going back to our hypothetical ``new'' patient, we get the following $95\%$ 
  interval estimates:
  \begin{align*}
    95\% CI_{\hat{Y}_{LDL}} &= [\Sexpr{confInt[1]}; \Sexpr{confInt[2]}]\\[8pt]
    95\% PI &= [\Sexpr{predInt[1]}; \Sexpr{predInt[2]}]
  \end{align*}
  \vx{-12}
  \begin{itemize}
  \item We can be 95\% confident that the \underline{average \emph{LDL}} of 
    patients with $BP = \Sexpr{x1}$, $gluc = \Sexpr{x2}$, and $BMI = \Sexpr{x3}$
    will be somewhere between \Sexpr{confInt[1]} and \Sexpr{confInt[2]}.
    \vb
  \item We can be 95\% confident that the \underline{\emph{LDL} of a specific 
    patient} with $BP = \Sexpr{x1}$, $gluc = \Sexpr{x2}$, and $BMI = \Sexpr{x3}$
    will be somewhere between \Sexpr{predInt[1]} and \Sexpr{predInt[2]}.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Specifying Predictive Models}
  
  When focused on inferences about regression coefficients, we care very much
  about the predictors entered into the model.  
  \vb
  \begin{itemize}
  \item Partial regression coefficients must be interpreted as controlling for
    all other predictors. 
  \end{itemize}
  \vb
  \pause
  When focused on prediction, we often don't care as much about the
  specific variables that enter the model.  
  \vb
  \begin{itemize}
  \item We prefer whatever set of features produces the best predictive
    performance.  
    \vb
  \item We may want to know which are the ``best'' predictors.
    \vc
    \begin{itemize}
    \item We usually want the data to ``give'' us this answer.
    \end{itemize}
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{\textsc{Aside}: Polynomial Regression}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      We may hypothesize a curvilinear relationship between $X$ and $Y$.
      \vc
      \begin{itemize}
      \item Polynomial regression adds powered transformations of the
        predictors into the model.
        \vc
      \item Polynomial terms (i.e., power terms) model curvature in the
        relationships.
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
p5 <- ggplot(data = Cars93, mapping = aes(x = Horsepower, y = MPG.city)) +
    theme_classic()
p6 <- p5 + geom_point() +
    theme(text = element_text(family = "Courier", size = 16))
p6 + xlab("Horsepower") + ylab("MPG") + ylim(c(10, 50))
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{\textsc{Aside}: Polynomial Regression}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      Polynomials are one way to model curvilinear relationships.
      \vb
      \begin{itemize}  
      \item {\color{blue}$\hat{Y}_{mpg} = \hat{\beta}_0 + \hat{\beta}_1 X_{hp}$}
        \vb
      \item {\color{red}$\hat{Y}_{mpg} = \hat{\beta}_0 + \hat{\beta}_1 X_{hp} + 
        \hat{\beta}_2 X_{hp}^2$}
        \vb
      \item {\color{violet}$\hat{Y}_{mpg} = \hat{\beta}_0 + \hat{\beta}_1 X_{hp} + 
        \hat{\beta}_2 X_{hp}^2 + \hat{\beta}_3 X_{hp}^3$}
      \end{itemize}
    \end{column}
    
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
p6 + geom_smooth(method = "lm", formula = y ~ x, se = FALSE) + 
    geom_smooth(method  = "lm", 
                formula = y ~ x + I(x^2), 
                se      = FALSE, 
                colour  = "red") +
    geom_smooth(method  = "lm", 
                formula = y ~ x + I(x^2) + I(x^3), 
                se      = FALSE, 
                colour  = "purple") +
    xlab("Horsepower") + 
    ylab("MPG") +
    ylim(c(10, 50))
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Evaluating Predictive Performance}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      How do we assess ``good'' prediction?
      \vb
      \begin{itemize}
      \item Can we simply find the model that best predicts the data used to
        train the model?  
        \vb
      \item What are we trying to do when building a predictive model?  
        \vb
      \item Can we quantify this objective with some type of fit measure?
      \end{itemize}
  
      \end{column}
    
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
nObs <- 10
r2   <- 0.9

x   <- rangeNorm(rnorm(nObs), 18, 60)
eta <- 0.035 * x + 0.015 * x^2
sig <- (var(eta) / r2) - var(eta)
y   <- eta + rnorm(nObs, 0, sqrt(sig))

x2   <- rangeNorm(rnorm(nObs), 18, 60)
eta2 <- 0.035 * x2 + 0.015 * x2^2
sig2 <- (var(eta2) / r2) - var(eta2)
y2   <- eta2 + rnorm(nObs, 0, sqrt(sig2))

form1 <- as.formula(
    paste0("y ~ x + ", paste0("I(x^", c(2 : 10), ")", collapse = " + "))
)

p1 <- gg0(x = x, y = y) + xlab("Age") + ylab("Mortality Risk")
p1
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Different Possible Models}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
        
<<echo = FALSE, out.width = "0.6\\textwidth", message = FALSE>>=
p1 + geom_smooth(method = "lm", se = FALSE)
@ 

\end{column}

\begin{column}{0.5\textwidth}
  
<<echo = FALSE, out.width = "0.6\\textwidth">>=
p1 + geom_smooth(method = "lm", se = FALSE, formula = y ~ x + I(x^2))
@ 

\end{column}
\end{columns}

  \va
  
\begin{columns}
  \begin{column}{0.5\textwidth}
    
<<echo = FALSE, out.width = "0.6\\textwidth">>=
p1 + geom_smooth(method = "lm", se = FALSE, formula = y ~ x + I(x^2) + I(x^3))
@ 

\end{column}
  
\begin{column}{0.5\textwidth}
  
<<echo = FALSE, out.width = "0.6\\textwidth">>=
p1 + geom_smooth(method  = "lm",
                 se      = FALSE,
                 formula = y ~ x + I(x^2) + I(x^3) + I(x^4))
@ 

\end{column}
\end{columns}

\end{frame} 
  
%------------------------------------------------------------------------------%

\begin{frame}{Over-fitting}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      We can easily go too far.
      \vb
      \begin{itemize}
      \item Enough polynomial terms will exactly replicate any data.  
        \vb
      \item Is this what we're trying to do?  
        \vb
      \item What kind of issues arise in the extreme case?
      \end{itemize}
      
    \end{column}
    
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, warning = FALSE>>=
p1 + geom_smooth(method = "lm", se = FALSE, formula = form1)
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Consequences of Over-fitting}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      Should we be pleased to be able to perfectly predict mortality risk? 
      \vb
      \begin{itemize}
      \item Is our model useful?
        \vc
      \item What happens if we try to apply our fitted model to new data?
      \end{itemize}
      
    \end{column}
    
    \pause
    
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, warning = FALSE>>=
p2 <- gg0(x = x2, y = y2) + xlab("Age") + ylab("Mortality Risk")
p2 + geom_smooth(data    = data.frame(x, y),
                 method  = "lm",
                 se      = FALSE,
                 formula = form1)
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Correct Fit}
  
  Let's try something a bit more reasonable.\\
  \vb
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
p1 + geom_smooth(method = "lm", se = FALSE, formula = y ~ x + I(x^2))
@ 

\end{column}

\begin{column}{0.5\textwidth}
  
<<echo = FALSE, warning = FALSE>>=
p2 + geom_smooth(data    = data.frame(x, y),
                 method  = "lm",
                 se      = FALSE,
                 formula = y ~ x + I(x^2)
                 )
@ 

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{A Sensible Goal}
  
  Our goal is to train a model that can best predict \emph{new data}.
  \vc
  \begin{itemize}
  \item The predictive performance on the training data is immaterial.
  \item We can always fit the training data arbitrarily well.
  \item Fit to the training data will always be at-odds with fit to future data.
  \end{itemize}
  \vc
  This conflict is the driving force behind the \emph{bias-variance trade-off}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Model Fit for Prediction}
  
  When assessing predictive performance, we will most often use the \emph{mean
    squared error} (MSE) as our criterion.
  \vb
  \begin{align*}
    MSE &= \frac{1}{N} \sum_{n = 1}^N \left(Y_n - \hat{Y}_n\right)^2\\
    &= \frac{1}{N} \sum_{n = 1}^N \left(Y_n - \hat{\beta}_0 - 
    \sum_{p = 1}^P \hat{\beta}_p X_{np} \right)^2\\
    &= \frac{RSS}{N}
  \end{align*}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Training vs. Test MSE}
  
  The MSE on the preceding slide (i.e., the only MSE we've considered, so far) 
  is computing entirely from training data.
  \vb
  \begin{itemize}
  \item \emph{Training MSE}
  \end{itemize}
  \vb
  What we want is a measure of fit to new, \emph{testing} data.
  \vb
  \begin{itemize}
  \item \emph{Test MSE}
    \vb
  \item Given $M$ new observations $\{Y_m, X_{m1}, X_{m2}, \ldots, X_{mP}\}$, 
    and a fitted regression model, $f(\mathbf{X})$, defined by the 
    coefficients $\{\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, \ldots, 
    \hat{\beta}_P\}$, the \emph{Test MSE} is given by:
    \begin{align*}
      MSE &= \frac{1}{M} \sum_{m = 1}^M \left(Y_m - \hat{\beta}_0 - 
      \sum_{p = 1}^P \hat{\beta}_p X_{mp} \right)^2\\
    \end{align*}
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Training vs. Test MSE}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      \textcolor{red}{Training MSE} will always decrease in response to 
      increased model complexity.  
      \vb
      \begin{itemize}
      \item Note the red line in the plot.
      \end{itemize}
      \vb
      \textcolor{blue}{Test MSE} will reach a minimum at some ``optimal'' level 
      of model complexity.
      \vb
      \begin{itemize}
      \item Further complicating the model will increase Test MSE.
        \vb
      \item Note the blue line.
      \end{itemize}
      
    \end{column}
    
    \begin{column}{0.5\textwidth}
      
<<"mse_fig", echo = FALSE, cache = TRUE>>=
maxPow <- 6
nObs   <- 100
nReps  <- 500

outList1 <- outList2 <- list()
for(rp in 1 : nReps) {
    x  <- rnorm(nObs)
    x2 <- rnorm(nObs)
    
    eta  <- x + x^2 + x^3 + x^4
    eta2 <- x2 + x2^2 + x2^3 + x2^4
    
    s2  <- (var(eta) / r2) - var(eta)
    s22 <- (var(eta2) / r2) - var(eta2)
    
    y  <- eta + rnorm(nObs, 0, sqrt(s2))
    y2 <- eta2 + rnorm(nObs, 0, sqrt(s22))
    
    form1 <- "y ~ 1"
    
    mse1 <- mse2 <- rep(NA, maxPow)
    for(p in 1 : maxPow) {
        form1 <- paste(form1, paste0("I(x^", p, ")"), sep = " + ")
        
        fit <- lm(as.formula(form1))
        
        yHat1 <- predict(fit)
        yHat2 <- predict(fit, newdata = data.frame(x = x2))
        
        mse1[p] <- crossprod(y - yHat1) / nObs
        mse2[p] <- crossprod(y2 - yHat2) / nObs
    }
    outList1[[rp]] <- mse1
    outList2[[rp]] <- mse2
}

mse1 <- colMeans(do.call(rbind, outList1))
mse2 <- colMeans(do.call(rbind, outList2))

p1 <- gg0(x = c(1 : maxPow), y = mse1, points = FALSE)
p1 + geom_point(colour = "red") +
    geom_line(colour = "red") +
    geom_point(mapping = aes(x = c(1 : maxPow), y = mse2), col = "blue") +
    geom_line(mapping = aes(x = c(1 : maxPow), y = mse2), col = "blue") +
    xlab("Polynomial Order") +
    ylab("MSE")
@ 

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Training vs. Test MSE}

  In the last lecture, we compared the following models:
  \begin{align}
    Y_{BP} &= \beta_0 + \beta_1 X_{age} + \beta_2 X_{LDL} + \beta_3 X_{HDL} + 
    \beta_4 X_{BMI} + \varepsilon \label{fullMod}\\
    Y_{BP} &= \beta_0 + \beta_1 X_{age} + \beta_2 X_{BMI} + 
    \varepsilon \label{resMod}
  \end{align}
  \vx{-12}
  \begin{itemize}
  \item The $\Delta R^2$ test suggested that the loss in fit between Model 
    \ref{fullMod} and Model \ref{resMod} was trivial.
    \vc
  \item The Training MSE values suggested that Model \ref{fullMod} should be 
    preferred.
  \end{itemize}
  \vb
  What happens when we do the comparison based on Test MSE instead of Training 
  MSE?
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Training vs. Test MSE}
  
<<>>=
set.seed(235711)

## Read in the data:
dDat <- readRDS("../data/diabetes.rds") 

## Split data into training and testing sets:
ind  <- sample(1 : nrow(dDat))
dat0 <- dDat[ind[1 : 400], ]   # Training data
dat1 <- dDat[ind[401 : 442], ] # Testing data

## Fit the models:
outF <- lm(bp ~ age + bmi + ldl + hdl, data = dat0)
outR <- lm(bp ~ age + bmi, data = dat0)

## Compute training MSEs:
trainMseF <- MSE(y_pred = predict(outF), y_true = dat0$bp)
trainMseR <- MSE(y_pred = predict(outR), y_true = dat0$bp)
@ 

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[fragile]{Training vs. Test MSE}
  
<<>>=
## Compute testing MSEs:
testMseF <- MSE(y_pred = predict(outF, newdata = dat1), 
                y_true = dat1$bp)
testMseR <- MSE(y_pred = predict(outR, newdata = dat1), 
                y_true = dat1$bp)
@ 

Compare the two approaches:

<<echo = FALSE, results = "asis">>=
tab1 <- xtable(
    matrix(c(trainMseF, testMseF, trainMseR, testMseR), 
           ncol = 2, 
           dimnames = list(c("Train", "Test"), c("Full", "Restricted"))
           ),
    caption = "MSE Values")

print(tab1, digits = 2, booktabs = TRUE)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Cross Validation}

  To train a model that best predicts new data, we use \emph{cross-validation}
  to choose the best model from a pool of candidates.  
  \vb
  \begin{itemize}
  \item Given a set $\mathcal{F} = \{f_1(\mathbf{X}), f_2(\mathbf{X}), \ldots, 
    f_J(\mathbf{X})\}$ of $J$ competing models, the simplest form of 
    cross-validation entails the following:
    \vb
    \begin{enumerate}
    \item Split the sample into two, disjoint sub-samples:
      \begin{itemize}
      \item \emph{Training} data
      \item \emph{Testing} data
      \end{itemize}
      \vc
    \item Estimate a candidate model, $f_j(\mathbf{X})$, on the training data. 
      \label{train}
      \vb
    \item Check the predictive performance of $\hat{f}_j(\mathbf{X})$ on the 
      testing data. \label{test}
      \vb
    \item Repeat Steps \ref{train} and \ref{test} for all candidate models in 
      $\mathcal{F}$.
      \vb
    \item Pick the $\hat{f}_j(\mathbf{X})$ that best predicts the testing data.
    \end{enumerate}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Estimating Prediction Error}
  
  The split-sample cross-validation scheme described above will underestimate 
  prediction error.
  \vc
  \begin{itemize}
  \item The same testing data are re-used to estimate the relative prediction 
    error of the candidate models.
    \vc
  \item If we need a good estimate of our final model's prediction error, we 
    need to complicate the split-sample procedure slightly:
    \pause
    \vc
    \begin{enumerate}
    \item Split the sample into \emph{three} disjoint subsets:
      \begin{itemize}
      \item \emph{Training} data, \emph{validation} data, and \emph{testing} 
        data
      \end{itemize}
      \vc
    \item Use the training and validation data to choose the best model via the 
      cross-validation procedure described earlier.
      \vc
    \item Fit the chosen model to the combined training and validation data. 
      \label{fitStep}
      \vc
    \item Use the Step \ref{fitStep} model to generate predictions from the 
      testing data.
      \vc
    \item Use the test-set predictions to estimate prediction error.
    \end{enumerate}
  \end{itemize}
  
\end{frame}
  
\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Estimating Prediction Error}
  
<<>>=
set.seed(235711)

## Split data into training, testing, and validation sets:
ind  <- sample(1 : nrow(dDat))
dat0 <- dDat[ind[1 : 350], ]   # Training data
dat1 <- dDat[ind[351 : 400], ] # Validation data
dat2 <- dDat[ind[401 : 442], ] # Testing data

## Fit models to training data:
outF <- lm(bp ~ age + bmi + ldl + hdl, data = dat0)
outR <- lm(bp ~ age + bmi, data = dat0)
@ 

\pagebreak

<<>>=
## Compute validation MSEs:
validMseF <- MSE(y_pred = predict(outF, newdata = dat1), 
                 y_true = dat1$bp)
validMseR <- MSE(y_pred = predict(outR, newdata = dat1), 
                 y_true = dat1$bp)

## Use validation MSEs to choose the best model:
validMseF
validMseR
@ 

\pagebreak

<<echo = FALSE>>=
useF <- validMseF < validMseR
@

<<>>=
## Pool training and validation data:
dat01 <- rbind(dat0, dat1)
@ 

<<echo = useF, eval = useF>>=
## Estimate the chosen model:
outC <- lm(bp ~ age + bmi + ldl + hdl, data = dat01)
@ 

<<echo = !useF, eval = !useF>>=
## Estimate the chosen model:
outC <- lm(bp ~ age + bmi, data = dat01)
@ 

<<>>=
## Estimate prediction error using the testing data:
predErr <- MSE(y_pred = predict(outC, newdata = dat2), 
               y_true = dat2$bp)

## This value can be reported as our best estimate of the 
## prediction error:
predErr
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Different Flavors of Cross-Validation}
  
  In practice, the split-sample cross-validation procedure describe above can be 
  highly variable.
  \vc
  \begin{itemize}
  \item The solution is highly sensitive to the way the sample is split because 
    each model is only training once.
  \end{itemize}
  \vb 
  Split-sample cross-validation can also be wasteful.
  \vc
  \begin{itemize}
  \item We don't need to set aside an entire chunk of data for validation.
  \end{itemize}
  \vb 
  In most cases, we will want to employ a slightly more complex flavor of
  cross-validation: 
  \vc
  \begin{itemize}
  \item \emph{K-fold cross-validation}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{\emph{K}-Fold Cross-Validation}
  
  \begin{enumerate}
  \item If you need to estimate prediction error, set aside a testing set.
    \vb
  \item Partition the (remaining) data into $K$ disjoint subsets $C_k = C_1, C_2, 
    \ldots, C_K$.  
    \vb
  \item Conduct $K$ training replications. 
    \vb
    \begin{itemize}
    \item For each training replication, collapse $K - 1$ partitions into 
      a set of training data. 
      \vb
    \item Compute the validation MSE for the $k$th partition, $MSE_k$, by using
      subset $C_k$ as the validation data for the $k$th fitted model.
    \end{itemize}
    \vb
  \item Compute the overall \kfold~error as:
  \end{enumerate}
  \begin{align*}
    CVE = \sum_{k = 1}^K \frac{N_k}{N}MSE_k,
  \end{align*}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{$K$-Fold Cross-Validation}

<<eval = FALSE>>=
getCve <- function(model, data, K, part) {
    ## Loop over K repetitions:
    mse <- c()
    for(k in 1 : K) {
        ## Partition data:
        train <- data[part != k, ]
        valid <- data[part == k, ]
        
        ## Fit model, generate predictions, and save MSE:
        fit    <- lm(model, data = train)
        pred   <- predict(fit, newdata = valid)
        mse[k] <- MSE(y_pred = pred, 
                      y_true = valid[ , dvName(fit)])
    }
    ## Return the CVE:
    mean(mse)
}
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{$K$-Fold Cross-Validation}

<<eval = FALSE>>=
## Do K-Fold Cross-Validation with lm():
cv.lm <- function(data, models, K) {
    ## Create a partition vector:
    part <- rep(1 : K, nrow(data) / K)
    
    ## Permute the partition vector:
    part <- sample(part)
    
    ## Apply over candidate models:
    sapply(models, getCve, data = data, K = K, part = part)
}
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{$K$-Fold Cross-Validation}
  
  So how does \kfold~perform in our $BP$ prediction task?
 
<<>>=
cvOut <- cv.lm(data   = dDat,
               models = c("bp ~ age + bmi + ldl + hdl",
                          "bp ~ age + bmi"),
               K      = 10)
cvOut
@
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%
  
\begin{frame}{}
  
  \begin{center}
    \Huge{\textsc{Missing Data Imputation}}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Imputation is Just Prediction}
  
  In Lecture 3, you heard a bit about missing data imputation.
  \begin{itemize}
  \item Multiple imputation is one of the best ways to treat missing data.
  \end{itemize}
  \vb
  Imputation is nothing more than a type of prediction.
  \begin{enumerate}
  \item Train a model on the observed parts of the data, $Y_{obs}$.
    \begin{itemize}
    \item Train the imputation model.
    \end{itemize}
  \item Predict the missing values, $Y_{mis}$.
    \begin{itemize}
    \item Generate imputations.
    \end{itemize}
  \item Replace the missing values with these predictions.
    \begin{itemize}
    \item Impute the missing data.
    \end{itemize}
  \end{enumerate}
  \vb
  Imputation can be used to support either prediction or inference.
  \begin{itemize}
  \item Our goals will dictate what type of imputation we need to do.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[shrink = 5]{Levels of Uncertainty Modeling}

  \citet{vanBuuren:2012} provides a very useful classification of different 
  imputation methods:
  \vb
  \begin{enumerate}
  \item Simple Prediction
    \begin{itemize}
    \item The missing data are naively filled with predicted values from some 
      regression equation.
    \item All uncertainty is ignored.
    \end{itemize}
    \vb
  \item Prediction + Noise
    \begin{itemize}
    \item A random residual error is added to each predicted value to create the 
      imputations.
    \item Only uncertainty in the predicted values is modeled.
    \item The imputation model itself is assumed to be correct and error-free.
    \end{itemize}
    \vb
  \item Prediction + Noise + Model Error
    \begin{itemize}
    \item Uncertainty in the imputation model itself is also modeled.
    \item Only way to get fully proper imputations in the sense of 
      \citet{rubin:1987}.
    \end{itemize}
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[shrink = 5]{Do we really need to worry?}

  The arguments against single imputation can seem archaic and petty. Do we 
  really need to worry about this stuff?\\  
  \pause
  \vc
  \begin{itemize}
  \item YES!!! (At least if you care about inference)\\
  \end{itemize}
  \vb
  The following are results from a simple Monte Carlo simulation:
  
<<echo = FALSE, results = "asis">>=
simRes <- readRDS(paste0(dataDir, "simResMat.rds"))

simResTab <- 
    xtable(simRes, 
           caption = "Mean Correlation Coefficients and Type I Error Rates",
           digits  = 3,
           align   = c("r", rep("c", 4))
           )

print(simResTab, scale = 0.8, booktabs = TRUE)
@ 

\pause
\vx{-12}
\begin{itemize}
\item Conditional mean substitution overestimates the correlation effect.
  \vc
\item Both single imputation methods inflate Type I error rates.
  \vc
\item MI provides unbiased point estimates and accurate Type I error rates.
\end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Simulate Some Toy Data}
  
<<>>=
nObs <- 1000 # Sample Size
pm   <- 0.3  # Proportion Missing

sigma <- matrix(c(1.0, 0.5, 0.0,
                  0.5, 1.0, 0.3,
                  0.0, 0.3, 1.0),
                ncol = 3)

dat0 <- as.data.frame(rmvnorm(nObs, c(0, 0, 0), sigma))
colnames(dat0) <- c("y", "x", "z")
@ 

\pagebreak

<<>>=
## Impose MAR Nonresponse:
dat1 <- dat0
rVec <- with(dat1, x < quantile(x, probs = pm))

dat1[rVec, "y"] <- NA

## Subset the data:
yMis <- dat1[rVec, ]
yObs <- dat1[!rVec, ]
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Look at the data.}
  
\begin{columns}
\begin{column}{0.5\textwidth}
      
\begin{onlyenv}<1>
<<>>=
round(head(dat0, n = 5), 3)
@ 
\end{onlyenv}

\begin{onlyenv}<2>
<<>>=
round(head(dat1, n = 5), 3)
@
\end{onlyenv}
      
\end{column}
\begin{column}{0.5\textwidth}
      
\only<1>{
<<echo = FALSE, out.width = "90%">>=
with(dat0, gg0(x = x, y = y))
@
}
\only<2>{
<<echo = FALSE, out.width = "90%">>=
p1 <- with(yObs, gg0(x = x, y = y)) +
    geom_point(mapping = aes(x = x, y = y), data = dat0[rVec, ], color = "gray")
p1
@ 
}

\end{column}
\end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Expected Imputation Model Parameters}
   
\begin{columns}
\begin{column}{0.5\textwidth}

<<>>=
lsFit <- lm(y ~ x + z, 
            data = yObs)

beta  <- coef(lsFit)
sigma <- summary(lsFit)$sigma

as.matrix(beta)
sigma
@ 

\end{column}
\begin{column}{0.5\textwidth}
    
<<echo = FALSE, out.width = "90%">>=
fit0 <- lm(y ~ x, data = yObs)
b0   <- coef(fit0)
s0   <- summary(fit0)$sigma

p2 <- p1 + 
    geom_abline(intercept = b0[1], slope = b0[2], color = "blue", lwd = 1)
p2
@ 
  
\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Conditional Mean Substitution}

\begin{columns}
\begin{column}{0.5\textwidth}

<<>>=
## Generate imputations:
imps <- beta[1] + 
    beta[2] * yMis[ , "x"] + 
    beta[3] * yMis[ , "z"]

## Fill missing cells in Y:
dat1[rVec, "y"] <- imps

round(head(dat1, n = 5), 3)
@ 

\end{column}
\begin{column}{0.5\textwidth}

<<echo = FALSE, out.width = "90%">>=
imps <- predict(fit0, newdata = yMis)
p2 + geom_point(mapping = aes(x = yMis[, "x"], y = imps),  color = "red")
@ 
  
\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Stochastic Regression Imputation}
   
\begin{columns}
\begin{column}{0.52\textwidth}
  
<<>>=
## Generate imputations:
imps <- imps + 
    rnorm(nrow(yMis), 0, sigma)

## Fill missing cells in Y:
dat1[rVec, "y"] <- imps

round(head(dat1, n = 5), 3)
@ 

\end{column}
\begin{column}{0.5\textwidth}
  
<<echo = FALSE, out.width = "90%">>=
imps <- imps + rnorm(length(imps), 0, s0)
p2 + geom_point(mapping = aes(x = yMis[, "x"], y = imps),  color = "red")
@ 
  
\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Flavors of MI}
  
  MI simply repeats a single regression imputation $M$ times.
  \begin{itemize}
  \item The specifics of the underlying regression imputation are important.
  \end{itemize}
  \vb
  \pause
  Simply repeating the stochastic regression imputation procedure described 
  above won't suffice.
  \begin{itemize}
  \item Still produces too many Type I errors
  \end {itemize}
  
<<echo = FALSE, results = "asis">>=
simRes2 <- readRDS(paste0(dataDir, "simResMat2.rds"))

simResTab2 <- 
    xtable(simRes2, 
           caption = "Mean Correlation Coefficients and Type I Error Rates",
           digits  = 3,
           align   = c("r", rep("c", 3))
           )

print(simResTab2, scale = 0.8, booktabs = TRUE) 
@

\vx{-16}
\begin{itemize}
\item Type I error rates for PN-Type MI are much better than they were for 
  single stochastic regression imputation, but they're still too high.
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Proper MI}
  
  The problems on the previous slide arise from using the same regression 
  coefficients to create each of the $M$ imputations.
  \begin{itemize}
  \item Implies that you're using the ``correct'' coefficients.
    \vb
  \item This assumption is plainly ridiculous.
    \begin{itemize}
    \item Any estimated regression line is only a guess of the true regression
      line.
      \vc
    \item Hence the standard errors of the coefficients.
    \end{itemize}
    \vb
    \pause
  \item Proper MI also models uncertainty in the regression coefficients used to 
    create the imputations.
    \begin{itemize}
    \item A different set of coefficients is randomly sampled (using Bayesian 
      simulation) to create each of the $M$ imputations.
      \vc
    \item The tricky part about implemented MI is deriving the distributions 
      from which to sample these coefficients.
    \end{itemize}
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Visualizing MI}
  
<<echo = FALSE, cache = TRUE>>=
## Estimate the imputation model:
nSams <- 100000
preds <- cbind(1, yObs$x)
df    <- nrow(yObs) - length(b0)

sigmaScale <- (1 / df) * crossprod(yObs$y - preds %*% b0)
sigmaSams  <- rinvchisq(nSams, df = df, scale = sigmaScale)

betaVarDenom <- solve(crossprod(preds))

betaSams <- matrix(NA, nSams, length(b0))
for(m in 1 : nSams) {
    betaSigma <- sigmaSams[m] * betaVarDenom
    betaSams[m, ] <- rmvnorm(1, mean = b0, sigma = betaSigma)
}

## Sample parameters:
index  <- sample(1 : nSams, 3)
betas  <- betaSams[index, ]
sigmas <- sigmaSams[index]

## Estimate posterior densities:
dS <- density(sigmaSams)
dB1 <- density(betaSams[ , 2])
@ 

Use Bayesian simulation to estimate posterior distributions for the imputation 
model parameters:\\

\vb

\begin{columns}
\begin{column}{0.5\textwidth}

<<echo = FALSE, out.width = "90%">>=
gg0(x = dS$x, y = dS$y, points = FALSE) + 
    geom_line() + 
    xlab("Residual Variance") + 
    ylab("Density")
@ 

\end{column}
\begin{column}{0.5\textwidth}
  
<<echo = FALSE, out.width = "90%">>=
gg0(x = dB1$x, y = dB1$y, points = FALSE) + 
    geom_line() + 
    xlab("Slope") + 
    ylab("Density")
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Visualizing MI}
  
\begin{columns}
\begin{column}{0.5\textwidth}

  \only<1>{
    Recall the incomplete data.
  }
  
  \only<2>{
    Sample values of $\beta_0$ and $\beta_1$:
    \begin{itemize}
    \item $\beta_0 = \Sexpr{round(betas[1, 1], 3)}$
    \item $\beta_1 = \Sexpr{round(betas[1, 2], 3)}$
    \end{itemize}
    \vb
    Define the predicted best-fit line:\\ 
    $\hat{Y}_{mis} = \Sexpr{round(betas[1, 1], 3)} + \Sexpr{round(betas[1, 2], 3)}X_{mis}$
  }
  
  \only<3>{
    Sample a value of $\sigma^2$:
    \begin{itemize}
    \item $\sigma^2 = \Sexpr{round(sigmas[1], 3)}$
    \end{itemize}
    \vb
    Generate imputations using the same procedure described in Single Stochastic 
    Regression Imputation:
    \begin{align*}
      Y_{imp} &= \hat{Y}_{mis} + \varepsilon\\
      \varepsilon &\sim \text{N}(0, \Sexpr{round(sigmas[1], 3)})
    \end{align*}
  }
  
  \only<4>{
    Sample values of $\beta_0$ and $\beta_1$:
    \begin{itemize}
    \item $\beta_0 = \Sexpr{round(betas[2, 1], 3)}$
    \item $\beta_1 = \Sexpr{round(betas[2, 2], 3)}$
    \end{itemize}
    \vb
    Define the predicted best-fit line:\\ 
    $\hat{Y}_{mis} = \Sexpr{round(betas[2, 1], 3)} + \Sexpr{round(betas[2, 2], 3)}X_{mis}$
  }
  
  \only<5>{
    Sample a value of $\sigma^2$:
    \begin{itemize}
    \item $\sigma^2 = \Sexpr{round(sigmas[2], 3)}$
    \end{itemize}
    \vb
    Generate imputations using the same procedure described in Single Stochastic 
    Regression Imputation:
    \begin{align*}
      Y_{imp} &= \hat{Y}_{mis} + \varepsilon\\
      \varepsilon &\sim \text{N}(0, \Sexpr{round(sigmas[2], 3)})
    \end{align*}
  }
  
  \only<6>{
    Sample values of $\beta_0$ and $\beta_1$:
    \begin{itemize}
    \item $\beta_0 = \Sexpr{round(betas[3, 1], 3)}$
    \item $\beta_1 = \Sexpr{round(betas[3, 2], 3)}$
    \end{itemize}
    \vb
    Define the predicted best-fit line:\\ 
    $\hat{Y}_{mis} = \Sexpr{round(betas[3, 1], 3)} + \Sexpr{round(betas[3, 2], 3)}X_{mis}$
  }
  
  \only<7>{
    Sample a value of $\sigma^2$:
    \begin{itemize}
    \item $\sigma^2 = \Sexpr{round(sigmas[3], 3)}$
    \end{itemize}
    \vb
    Generate imputations using the same procedure described in Single Stochastic 
    Regression Imputation:
    \begin{align*}
      Y_{imp} &= \hat{Y}_{mis} + \varepsilon\\
      \varepsilon &\sim \text{N}(0, \Sexpr{round(sigmas[3], 3)})
    \end{align*}
  }
  
\end{column}
\begin{column}{0.5\textwidth}
  
\only<1>{
<<echo = FALSE, out.width = "90%">>=
print(p1)
@
}
\only<2>{
<<echo = FALSE, out.width = "90%">>=
p3 <- p1 + 
    geom_abline(intercept = betas[1, 1], 
                slope     = betas[1, 2], 
                color     = "blue", 
                lwd       = 1)
p3
@
}
\only<3>{
<<echo = FALSE, out.width = "90%">>=
mPreds <- cbind(1, yMis$x)
imps   <- mPreds %*% betas[1, ] + rnorm(nrow(yMis), 0, sqrt(sigmas[1]))

p3 + geom_point(mapping = aes(x = yMis$x, y = imps), color = "red")
@
}
\only<4>{
<<echo = FALSE, out.width = "90%">>=
p3 <- p1 + 
    geom_abline(intercept = betas[1, 1], 
                slope     = betas[1, 2], 
                color     = "lightblue", 
                lwd       = 1) + 
    geom_abline(intercept = betas[2, 1], 
                slope     = betas[2, 2], 
                color     = "blue", 
                lwd       = 1)
p3
@
}
\only<5>{
<<echo = FALSE, out.width = "90%">>=
mPreds <- cbind(1, yMis$x)
imps   <- mPreds %*% betas[2, ] + rnorm(nrow(yMis), 0, sqrt(sigmas[2]))

p3 + geom_point(mapping = aes(x = yMis$x, y = imps), color = "red")
@
}
\only<6>{
<<echo = FALSE, out.width = "90%">>=
p3 <- p1 + 
    geom_abline(intercept = betas[2, 1], 
                slope     = betas[2, 2], 
                color     = "lightblue", 
                lwd       = 1) + 
    geom_abline(intercept = betas[3, 1], 
                slope     = betas[3, 2], 
                color     = "blue", 
                lwd       = 1)
p3

@
}
\only<7>{
<<echo = FALSE, out.width = "90%">>=
mPreds <- cbind(1, yMis$x)
imps   <- mPreds %*% betas[3, ] + rnorm(nrow(yMis), 0, sqrt(sigmas[3]))

p3 + geom_point(mapping = aes(x = yMis$x, y = imps), color = "red")
@
}

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[shrink = 5, fragile]{Doing Imputation in Practice}
  
  Each of the preceding approaches is available in the \textsf{R} package 
  \texttt{mice} \citep{mice}.
  
<<eval = FALSE>>=
## Conditional Mean Substitution:
miceOut1 <- mice(data   = missData,
                 m      = 1,
                 method = "norm.predict")
impDat1 <- complete(miceOut1)

## Stochastic Regression Imputation:
miceOut2 <- mice(data   = missData,
                 m      = 1,
                 method = "norm.nob")
impDat2 <- complete(miceOut2)

## Proper MI:
miceOut3 <- mice(data   = missData,
                 m      = 25,
                 method = "norm")
impList <- complete(miceOut3, "all")
@ 

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Doing MI-Based Analysis}
  
  An MI-based data analysis consists of three phases:
  \vb
  \begin{enumerate}
  \item The imputation phase \label{iStep}
    \begin{itemize}
    \item Replace missing values with $M$ plausible estimates.
    \item Produce $M$ completed datasets.
    \end{itemize}
    \vb
  \item The analysis phase \label{aStep}
    \begin{itemize}
    \item Estimate $M$ replicates of your analysis model.
    \item Fit the same model to each of the $M$ datasets from Step \ref{iStep}.
    \end{itemize}
    \vb
  \item The pooling phase
    \begin{itemize}
    \item Combine the $M$ sets of parameter estimates and standard errors from 
      Step \ref{aStep} into a single set of MI estimates.
    \item Use these pooled parameter estimates and standard errors for 
      inference.
    \end{itemize}
  \end{enumerate}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Schematic Representation of MI-Bases Analysis}

  \begin{center}
    \includegraphics<1>[width = 0.75\textwidth]{figures/mi_schematic0.pdf}
    \includegraphics<2>[width = 0.75\textwidth]{figures/mi_schematic1.pdf}
    \includegraphics<3>[width = 0.75\textwidth]{figures/mi_schematic2.pdf}
    \includegraphics<4>[width = 0.75\textwidth]{figures/mi_schematic.pdf}
  \end{center}
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Pooling MI Estimates}
  
  \citet{rubin:1987} formulated a simple set of pooling rules for MI estimates.
  \vb
  \begin{itemize}
  \item The MI point estimate of some interesting quantity, $Q^*$, is simply 
    the mean of the $M$ estimates, $\{\hat{Q}_m\}$:
    \begin{align*}
      Q^* &= \frac{1}{M} \sum_{m = 1}^M \hat{Q}_m\\
    \end{align*}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Pooling MI Estimates}
  
  The MI variability estimate, $T$, is a slightly more complex entity.
  \vb
  \begin{itemize}
  \item A weighted sum of the \emph{within-imputation} variance, $W$, and the 
    \emph{between-imputation} variance, $B$.
    \begin{align*}
      W &= \frac{1}{M} \sum_{m = 1}^M SE(\hat{Q}_m)^2\\
      B &= \frac{1}{M - 1} \sum_{m = 1}^M \left( \hat{Q}_m - Q^* \right)^2\\
      T &= W + \left( 1 + M^{-1} \right) B\\ 
      &= W + B + \frac{B}{M}
    \end{align*}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Inference with MI Estimates}
  
  After computing $Q^*$ and $T$, we combine them in the usual way to get test 
  statistics and confidence intervals.
  \begin{align*}
    t &= \frac{Q^* - Q_0}{\sqrt{T}}\\
    CI &= Q^* \pm t_{crit} \sqrt{T}
  \end{align*}
  
  We must take care with our \emph{df}, though.
  \begin{align*}
    df = (M - 1) \left[1 + \frac{W}{\left(1 + M^{-1}\right)B}\right]^2
  \end{align*}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Fraction of Missing Information}
  
  In Lecture 4, we briefly discussed a very desirable measure of nonresponse: 
  \emph{fraction of missing information} (FMI).
  
  \begin{align*}
    FMI = \frac{r + \frac{2}{(df + 3)}}{r + 1} \approx \frac{(1 + M^{-1})B}{(1 + M^{-1})B + W} \rightarrow \frac{B}{B + W}
  \end{align*}
  where
  \begin{align*}
    r = \frac{(1 + M^{-1})B}{W}
  \end{align*}
  The FMI gives us a sense of how much the missing data (and their treatment) 
  have influenced our parameter estimates.
  \vc
  \begin{itemize}
  \item We should report the FMI for an estimated parameter along with other 
    ancillary statistics (e.g., t-tests, p-values, effect sizes, etc.).
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Prediction/Cross-Validation with MI Data}

  When doing an MI-based analysis, we generally want to pool results as late as 
  possible in the analytic process.
  \vc
  \begin{itemize}
  \item This pattern also holds when doing prediction/cross-validation with MI 
    data.
    \vc
  \item When doing prediction, we pool the $M$ sets of predictions.
    \begin{itemize}
    \item We don't generate predictions using the pooled parameters.
    \end{itemize}
    \vc
  \item When doing cross-validation, we pool the $M$ sets of MSE values.
    \begin{itemize}
    \item We don't generate MSE values using pooled predictions or parameters.
    \end{itemize}
  \end{itemize}
  
  \pagebreak
  
  To generate predictions from $M$ multiply imputed datasets, we would:
  \begin{enumerate}
  \item Train the model on each of the $M$ datasets separately.
  \item Generate $M$ sets of predictions from the $M$ models trained above.
  \item Average the $M$ sets of predictions into a single vector of predicted 
    values.
  \end{enumerate}
  \vb
  To run (K-fold) cross-validation with $M$ multiply imputed datasets, we need 
  to:
  \begin{enumerate}
  \item Split each of the $M$ imputed datasets.
  \item Run the cross-validation procedure separately on each of the $M$ imputed 
    datasets.
  \item Pool the final $M$ sets of MSE values.
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Conclusion}
  
  \begin{itemize}
  \item We can use a fitted regression model to generate predictions of new 
    outcomes.
    \vc
  \item We can get two different types of interval estimates around predictions: 
    \emph{confidence intervals} and \emph{prediction intervals}.
    \vc
  \item When building predictive models, we don't care that much about which 
    variables are used.
    \begin{itemize}
    \item We want to get the best possible predictions.
    \end{itemize}
    \vc
  \item When assessing predictive performance, we should consider 
    \emph{test-set} performance, not \emph{training-set} performance.
    \vc
  \item We can use (k-fold) cross-validation to tune our models and estimate 
    prediction error.
    
    \pagebreak
    
  \item Missing data imputation is a very useful type of prediction.
    \vc
  \item We can differentiate between three levels of uncertainty modeling in 
    imputation: \emph{P-}, \emph{PN-}, and \emph{PNE-type}.
    \begin{itemize}
    \item To achieve correct inferences, we need to use full PNE-type multiple 
      imputation.
    \end{itemize}
    \vc
  \item An MI-based analysis consists of three phases: \emph{imputation}, 
    \emph{analysis}, and \emph{pooling}.
    \vc
  \item We should report the FMI for any parameter estimated in an MI-based 
    analysis.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{References}
  
  \bibliographystyle{apacite}
  \bibliography{../../../literature/bibtexFiles/statMethRefs.bib,../../../literature/bibtexFiles/dissRefsList.bib}
  
\end{frame}

%------------------------------------------------------------------------------%

\end{document}

